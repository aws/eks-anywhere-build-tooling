From b59fb24b8a2ea390874887c486f684614babdd88 Mon Sep 17 00:00:00 2001
From: Prow Bot <prow@amazonaws.com>
Date: Wed, 16 Aug 2023 19:58:01 -0700
Subject: [PATCH 33/40] Add support for external etcd machines in Kind mapper

---
 .../client/cluster/objectgraph_test.go        |  2 +-
 .../dockermachinepool_controller_phases.go    |  2 +-
 .../controllers/dockermachine_controller.go   | 35 ++++++----
 .../docker/internal/docker/machine.go         | 69 +++++++++++++------
 4 files changed, 70 insertions(+), 38 deletions(-)

diff --git a/cmd/clusterctl/client/cluster/objectgraph_test.go b/cmd/clusterctl/client/cluster/objectgraph_test.go
index 2b92ba3bc..48b617339 100644
--- a/cmd/clusterctl/client/cluster/objectgraph_test.go
+++ b/cmd/clusterctl/client/cluster/objectgraph_test.go
@@ -1934,7 +1934,7 @@ func TestObjectGraph_DiscoveryByNamespace(t *testing.T) {
 		{
 			name: "two clusters with external force object, read only 1 cluster & both external objects",
 			args: args{
-				cluster: "cluster1", // read only from ns1
+				namespace: "ns1", // read only from ns1
 				objs: func() []client.Object {
 					objs := []client.Object{}
 					objs = append(objs, test.NewFakeCluster("ns1", "cluster1").Objs()...)
diff --git a/test/infrastructure/docker/exp/internal/controllers/dockermachinepool_controller_phases.go b/test/infrastructure/docker/exp/internal/controllers/dockermachinepool_controller_phases.go
index e455b931b..24597fa78 100644
--- a/test/infrastructure/docker/exp/internal/controllers/dockermachinepool_controller_phases.go
+++ b/test/infrastructure/docker/exp/internal/controllers/dockermachinepool_controller_phases.go
@@ -99,7 +99,7 @@ func createDockerContainer(ctx context.Context, name string, cluster *clusterv1.
 	}
 
 	log.Info("Creating container for machinePool", "name", name, "MachinePool", klog.KObj(machinePool))
-	if err := externalMachine.Create(ctx, dockerMachinePool.Spec.Template.CustomImage, constants.WorkerNodeRoleValue, machinePool.Spec.Template.Spec.Version, labels, dockerMachinePool.Spec.Template.ExtraMounts); err != nil {
+	if err := externalMachine.Create(ctx, dockerMachinePool.Spec.Template.CustomImage, constants.WorkerNodeRoleValue, machinePool.Spec.Template.Spec.Version, labels, dockerMachinePool.Spec.Template.ExtraMounts, false); err != nil {
 		return errors.Wrapf(err, "failed to create docker machine with name %s", name)
 	}
 	return nil
diff --git a/test/infrastructure/docker/internal/controllers/dockermachine_controller.go b/test/infrastructure/docker/internal/controllers/dockermachine_controller.go
index e9b987bc8..6bc384d31 100644
--- a/test/infrastructure/docker/internal/controllers/dockermachine_controller.go
+++ b/test/infrastructure/docker/internal/controllers/dockermachine_controller.go
@@ -295,7 +295,7 @@ func (r *DockerMachineReconciler) reconcileNormal(ctx context.Context, cluster *
 	if !externalMachine.Exists() {
 		// NOTE: FailureDomains don't mean much in CAPD since it's all local, but we are setting a label on
 		// each container, so we can check placement.
-		if err := externalMachine.Create(ctx, dockerMachine.Spec.CustomImage, role, machine.Spec.Version, docker.FailureDomainLabel(machine.Spec.FailureDomain), dockerMachine.Spec.ExtraMounts); err != nil {
+		if err := externalMachine.Create(ctx, dockerMachine.Spec.CustomImage, role, machine.Spec.Version, docker.FailureDomainLabel(machine.Spec.FailureDomain), dockerMachine.Spec.ExtraMounts, util.IsEtcdMachine(machine)); err != nil {
 			return ctrl.Result{}, errors.Wrap(err, "failed to create worker DockerMachine")
 		}
 	}
@@ -386,7 +386,7 @@ func (r *DockerMachineReconciler) reconcileNormal(ctx context.Context, cluster *
 			}()
 
 			// Run the bootstrap script. Simulates cloud-init/Ignition.
-			if err := externalMachine.ExecBootstrap(timeoutCtx, bootstrapData, format, version, dockerMachine.Spec.CustomImage); err != nil {
+			if err := externalMachine.ExecBootstrap(timeoutCtx, bootstrapData, format, version, dockerMachine.Spec.CustomImage, util.IsEtcdMachine(machine)); err != nil {
 				conditions.MarkFalse(dockerMachine, infrav1.BootstrapExecSucceededCondition, infrav1.BootstrapFailedReason, clusterv1.ConditionSeverityWarning, "Repeating bootstrap")
 				return ctrl.Result{}, errors.Wrap(err, "failed to exec DockerMachine bootstrap")
 			}
@@ -415,24 +415,29 @@ func (r *DockerMachineReconciler) reconcileNormal(ctx context.Context, cluster *
 	// set to true after a control plane machine has a node ref. If we would requeue here in this case, the
 	// Machine will never get a node ref as ProviderID is required to set the node ref, so we would get a deadlock.
 	if cluster.Spec.ControlPlaneRef != nil &&
-		!conditions.IsTrue(cluster, clusterv1.ControlPlaneInitializedCondition) {
+		!conditions.IsTrue(cluster, clusterv1.ControlPlaneInitializedCondition) &&
+		!util.IsEtcdMachine(machine) {
 		return ctrl.Result{RequeueAfter: 15 * time.Second}, nil
 	}
 
-	// Usually a cloud provider will do this, but there is no docker-cloud provider.
-	// Requeue if there is an error, as this is likely momentary load balancer
-	// state changes during control plane provisioning.
-	remoteClient, err := r.Tracker.GetClient(ctx, client.ObjectKeyFromObject(cluster))
-	if err != nil {
-		return ctrl.Result{}, errors.Wrap(err, "failed to generate workload cluster client")
-	}
-	if err := externalMachine.CloudProviderNodePatch(ctx, remoteClient, dockerMachine); err != nil {
-		if errors.As(err, &docker.ContainerNotRunningError{}) {
-			return ctrl.Result{}, errors.Wrap(err, "failed to patch the Kubernetes node with the machine providerID")
+	// In case of an etcd cluster, there is no concept of kubernetes node. So we can generate the node Provider ID and set it on machine spec directly
+	if !util.IsEtcdMachine(machine) {
+		// Usually a cloud provider will do this, but there is no docker-cloud provider.
+		// Requeue if there is an error, as this is likely momentary load balancer
+		// state changes during control plane provisioning.
+		remoteClient, err := r.Tracker.GetClient(ctx, client.ObjectKeyFromObject(cluster))
+		if err != nil {
+			return ctrl.Result{}, errors.Wrap(err, "failed to generate workload cluster client")
+		}
+		if err := externalMachine.CloudProviderNodePatch(ctx, remoteClient, dockerMachine); err != nil {
+			if errors.As(err, &docker.ContainerNotRunningError{}) {
+				return ctrl.Result{}, errors.Wrap(err, "failed to patch the Kubernetes node with the machine providerID")
+			}
+			log.Error(err, "Failed to patch the Kubernetes node with the machine providerID")
+			return ctrl.Result{RequeueAfter: 5 * time.Second}, nil
 		}
-		log.Error(err, "Failed to patch the Kubernetes node with the machine providerID")
-		return ctrl.Result{RequeueAfter: 5 * time.Second}, nil
 	}
+
 	// Set ProviderID so the Cluster API Machine Controller can pull it
 	providerID := externalMachine.ProviderID()
 	dockerMachine.Spec.ProviderID = &providerID
diff --git a/test/infrastructure/docker/internal/docker/machine.go b/test/infrastructure/docker/internal/docker/machine.go
index b354ed894..3f02a3508 100644
--- a/test/infrastructure/docker/internal/docker/machine.go
+++ b/test/infrastructure/docker/internal/docker/machine.go
@@ -48,6 +48,7 @@ import (
 	"sigs.k8s.io/cluster-api/test/infrastructure/docker/internal/provisioning/ignition"
 	"sigs.k8s.io/cluster-api/test/infrastructure/kind"
 	"sigs.k8s.io/cluster-api/util/patch"
+	versionutil "sigs.k8s.io/cluster-api/util/version"
 )
 
 var (
@@ -201,23 +202,35 @@ func (m *Machine) ContainerImage() string {
 }
 
 // Create creates a docker container hosting a Kubernetes node.
-func (m *Machine) Create(ctx context.Context, image string, role string, version *string, labels map[string]string, mounts []infrav1.Mount) error {
+func (m *Machine) Create(ctx context.Context, image string, role string, version *string, labels map[string]string, mounts []infrav1.Mount, isEtcdMachine bool) error {
 	log := ctrl.LoggerFrom(ctx)
 
 	// Create if not exists.
 	if m.container == nil {
 		var err error
+		var semVer semver.Version
+
+		// External etcd machines do not set a version field in the machine.Spec.Version.
+		// So we are parsing the Kubernetes semantic version from the Kind node tag and
+		// using that to get the Kind Mapping.
+		if isEtcdMachine {
+			nodeImageTag := strings.Split(image, ":")[1]
+			semVer, err = versionutil.ParseMajorMinorPatch(nodeImageTag)
+			if err != nil {
+				return errors.Wrap(err, "failed to parse semantic version from image tag")
+			}
+		} else {
+			// Parse the semver from the Spec.Version if not nil and get the KindMapping using the semver.
+			// NOTE: The KindMapping allows to select the most recent kindest/node image available, if any, as well as
+			// provide info about the mode to be used when starting the kindest/node image itself.
+			if version == nil {
+				return errors.New("cannot create a DockerMachine for a nil version")
+			}
 
-		// Get the KindMapping for the target K8s version.
-		// NOTE: The KindMapping allows to select the most recent kindest/node image available, if any, as well as
-		// provide info about the mode to be used when starting the kindest/node image itself.
-		if version == nil {
-			return errors.New("cannot create a DockerMachine for a nil version")
-		}
-
-		semVer, err := semver.Parse(strings.TrimPrefix(*version, "v"))
-		if err != nil {
-			return errors.Wrap(err, "failed to parse DockerMachine version")
+			semVer, err = semver.Parse(strings.TrimPrefix(*version, "v"))
+			if err != nil {
+				return errors.Wrap(err, "failed to parse DockerMachine version")
+			}
 		}
 
 		kindMapping := kind.GetMapping(semVer, image)
@@ -329,23 +342,37 @@ func (m *Machine) PreloadLoadImages(ctx context.Context, images []string) error
 }
 
 // ExecBootstrap runs bootstrap on a node, this is generally `kubeadm <init|join>`.
-func (m *Machine) ExecBootstrap(ctx context.Context, data string, format bootstrapv1.Format, version *string, image string) error {
+func (m *Machine) ExecBootstrap(ctx context.Context, data string, format bootstrapv1.Format, version *string, image string, isEtcdMachine bool) error {
 	log := ctrl.LoggerFrom(ctx)
 
 	if m.container == nil {
 		return errors.New("unable to set ExecBootstrap. the container hosting this machine does not exists")
 	}
 
-	// Get the kindMapping for the target K8s version.
-	// NOTE: The kindMapping allows to select the most recent kindest/node image available, if any, as well as
-	// provide info about the mode to be used when starting the kindest/node image itself.
-	if version == nil {
-		return errors.New("cannot create a DockerMachine for a nil version")
-	}
+	var err error
+	var semVer semver.Version
 
-	semVer, err := semver.Parse(strings.TrimPrefix(*version, "v"))
-	if err != nil {
-		return errors.Wrap(err, "failed to parse DockerMachine version")
+	// External etcd machines do not set a version field in the machine.Spec.Version.
+	// So we are parsing the Kubernetes semantic version from the Kind node tag and
+	// using that to get the Kind Mapping.
+	if isEtcdMachine {
+		nodeImageTag := strings.Split(image, ":")[1]
+		semVer, err = versionutil.ParseMajorMinorPatch(nodeImageTag)
+		if err != nil {
+			return errors.Wrap(err, "failed to parse semantic version from image tag")
+		}
+	} else {
+		// Parse the semver from the Spec.Version if not nil and get the KindMapping using the semver.
+		// NOTE: The KindMapping allows to select the most recent kindest/node image available, if any, as well as
+		// provide info about the mode to be used when starting the kindest/node image itself.
+		if version == nil {
+			return errors.New("cannot create a DockerMachine for a nil version")
+		}
+
+		semVer, err = semver.Parse(strings.TrimPrefix(*version, "v"))
+		if err != nil {
+			return errors.Wrap(err, "failed to parse DockerMachine version")
+		}
 	}
 
 	kindMapping := kind.GetMapping(semVer, image)
-- 
2.46.2

