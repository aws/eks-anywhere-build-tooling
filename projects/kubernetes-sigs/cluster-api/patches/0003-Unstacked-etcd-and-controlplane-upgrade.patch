From 1f30a6dc57093d0d70eea94fee29b55e5597d21d Mon Sep 17 00:00:00 2001
From: Rajashree Mandaogane <mandaor@amazon.com>
Date: Fri, 6 Aug 2021 17:16:39 -0700
Subject: [PATCH 03/41] Unstacked etcd and controlplane upgrade

Rename controlplane upgrade annotation variable

Add controlplane upgrade complete annotation to sync etcd to v1beta1

Check if controlplane is paused before patching it with paused annotation

Patch etcd cluster with upgrade complete annotation only after upgrade

After KCP upgrade is completed, the controller checks the condition "MachinesSpecUpToDate"
exists and marks it to true. The controller also updates the etcd cluster with an
annotation to indicate the controlplane upgrade is complete. But it should annotate
etcd cluster only if the MachineSpecUpToDate condition is False, since that will happen
only immediately after an upgrade. Without checking for false it will keep annotating
the etcd cluster on further reconcile calls.

cr: https://code.amazon.com/reviews/CR-55949234

Synchronize upgrade flow

Etcd machines need to be rolled out first so that KCP can get the
correct set of etcd endpoints. KCP rollout should be stalled till etcd upgrade
is done. Cluster controller may not be able to add the paused annotation on time
once updated spec is applied, since etcd and KCP controllers could have
started reconciling at the same time. So KCP checks if etcd cluster has the
upgrading annotation, and does not proceed further until the annotation is removed.
Once KCP has rolled out updated machines, it will add an annotation on the etcd
cluster to indicate that the KCP upgrade is complete. In absence of static etcd
endpoints, currently etcd controller retains older etcd members for KCP upgrade,
and once KCP upgrade complete annotation is set, the etcd controller will
remove these older etcd members

cr: https://code.amazon.com/reviews/CR-54933898

Improve external managed etcd to allow manual orchestration

Adds annotation to the KCP that when present, stops the cluster
controller from pausing/unpausing the KCP when etcdadm cluster is not
ready. We make this behavior opt-out to maintain compatibility with
previous clients that expect this orchestration by default.
---
 api/v1beta1/common_types.go                   |   7 +
 controllers/external/util.go                  |  29 ++
 .../kubeadm/api/v1beta1/condition_consts.go   |   8 +
 .../internal/controllers/controller.go        | 116 ++++++--
 .../internal/controllers/controller_test.go   | 209 +++++++++++++-
 .../kubeadm/internal/controllers/upgrade.go   |   6 +
 .../kubeadm/internal/workload_cluster.go      |   1 +
 .../kubeadm/internal/workload_cluster_etcd.go |   8 +
 .../controllers/cluster/cluster_controller.go |   2 +-
 .../cluster/cluster_controller_phases.go      | 104 +++----
 .../cluster/cluster_controller_test.go        | 257 ++++++++++++++++++
 .../machine_controller_noderef_test.go        |   6 +-
 internal/test/builder/builders.go             |  10 +
 internal/test/builder/etcd.go                 |  80 ++++++
 util/annotations/helpers.go                   |   5 +
 15 files changed, 778 insertions(+), 70 deletions(-)
 create mode 100644 internal/test/builder/etcd.go

diff --git a/api/v1beta1/common_types.go b/api/v1beta1/common_types.go
index b25985f1e..8b4902cd3 100644
--- a/api/v1beta1/common_types.go
+++ b/api/v1beta1/common_types.go
@@ -165,6 +165,9 @@ const (
 	// will receive the resulting object.
 	TopologyDryRunAnnotation = "topology.cluster.x-k8s.io/dry-run"
 
+	// ControlPlaneUpgradeCompletedAnnotation is set by the controlplane on the external etcd object after controlplane upgrade is completed.
+	ControlPlaneUpgradeCompletedAnnotation = "controlplane.cluster.x-k8s.io/upgrade-complete"
+
 	// ReplicasManagedByAnnotation is an annotation that indicates external (non-Cluster API) management of infra scaling.
 	// The practical effect of this is that the capi "replica" count should be passively derived from the number of observed infra machines,
 	// instead of being a source of truth for eventual consistency.
@@ -194,6 +197,10 @@ const (
 	// VariableDefinitionFromInline indicates a patch or variable was defined in the `.spec` of a ClusterClass
 	// rather than from an external patch extension.
 	VariableDefinitionFromInline = "inline"
+
+	// SkipControlPlanePauseManagedEtcdAnnotation indicates that the cluster controller should not pause or unpause
+	// the control plane after the managed etcd cluster becomes not-ready/ready.
+	SkipControlPlanePauseManagedEtcdAnnotation = "cluster.x-k8s.io/skip-pause-cp-managed-etcd"
 )
 
 // MachineSetPreflightCheck defines a valid MachineSet preflight check.
diff --git a/controllers/external/util.go b/controllers/external/util.go
index 7e0fd392a..5b6443c78 100644
--- a/controllers/external/util.go
+++ b/controllers/external/util.go
@@ -254,3 +254,32 @@ func GetExternalEtcdEndpoints(externalEtcd *unstructured.Unstructured) (string,
 
 	return endpoints, found, nil
 }
+
+func IsExternalEtcdUpgrading(externalEtcd *unstructured.Unstructured) (bool, error) {
+	annotations, hasAnnotations, err := unstructured.NestedStringMap(externalEtcd.Object, "metadata", "annotations")
+	if err != nil {
+		return false, errors.Wrapf(err, "failed to check if external etcd is undergoing upgrade %v %q", externalEtcd.GroupVersionKind(),
+			externalEtcd.GetName())
+	}
+
+	if !hasAnnotations {
+		return false, nil
+	}
+
+	_, hasUpgradingAnnotation := annotations["etcdcluster.cluster.x-k8s.io/upgrading"]
+	return hasUpgradingAnnotation, nil
+}
+
+func SetKCPUpdateCompleteAnnotationOnEtcdadmCluster(externalEtcd *unstructured.Unstructured) error {
+	annotations, hasAnnotations, err := unstructured.NestedStringMap(externalEtcd.Object, "metadata", "annotations")
+	if err != nil {
+		return errors.Wrapf(err, "failed to update external etcd annotation after controlplane upgrade completed %v %q", externalEtcd.GroupVersionKind(),
+			externalEtcd.GetName())
+	}
+
+	if !hasAnnotations {
+		annotations = make(map[string]string)
+	}
+	annotations[clusterv1.ControlPlaneUpgradeCompletedAnnotation] = "true"
+	return unstructured.SetNestedStringMap(externalEtcd.UnstructuredContent(), annotations, "metadata", "annotations")
+}
diff --git a/controlplane/kubeadm/api/v1beta1/condition_consts.go b/controlplane/kubeadm/api/v1beta1/condition_consts.go
index e9870d34c..adc1b2a0a 100644
--- a/controlplane/kubeadm/api/v1beta1/condition_consts.go
+++ b/controlplane/kubeadm/api/v1beta1/condition_consts.go
@@ -54,6 +54,14 @@ const (
 	// RollingUpdateInProgressReason (Severity=Warning) documents a KubeadmControlPlane object executing a
 	// rolling upgrade for aligning the machines spec to the desired state.
 	RollingUpdateInProgressReason = "RollingUpdateInProgress"
+
+	// ExternalEtcdEndpointsAvailable documents that the external etcd cluster's endpoints are available, and if KCP spec has changed
+	// then a KCP rollout can progress.
+	ExternalEtcdEndpointsAvailable clusterv1.ConditionType = "ExternalEtcdEndpointsAvailable"
+
+	// ExternalEtcdUndergoingUpgrade (Severity=Info) documents the external etcd cluster being used by current KCP object is
+	// undergoing an upgrade and that the etcd endpoints will change once the upgrade completes
+	ExternalEtcdUndergoingUpgrade = "ExternalEtcdUndergoingUpgrade"
 )
 
 const (
diff --git a/controlplane/kubeadm/internal/controllers/controller.go b/controlplane/kubeadm/internal/controllers/controller.go
index 9bdafa7ec..bc2da3e25 100644
--- a/controlplane/kubeadm/internal/controllers/controller.go
+++ b/controlplane/kubeadm/internal/controllers/controller.go
@@ -25,6 +25,7 @@ import (
 	"time"
 
 	"github.com/blang/semver/v4"
+	"github.com/go-logr/logr"
 	"github.com/pkg/errors"
 	corev1 "k8s.io/api/core/v1"
 	apierrors "k8s.io/apimachinery/pkg/api/errors"
@@ -174,28 +175,12 @@ func (r *KubeadmControlPlaneReconciler) Reconcile(ctx context.Context, req ctrl.
 	}
 
 	if cluster.Spec.ManagedExternalEtcdRef != nil {
-		etcdRef := cluster.Spec.ManagedExternalEtcdRef
-		externalEtcd, err := external.Get(ctx, r.Client, etcdRef, cluster.Namespace)
+		managedEtcdResult, err := r.updateManagedExternalEtcdEndpoints(ctx, log, patchHelper, cluster, kcp)
 		if err != nil {
 			return ctrl.Result{}, err
 		}
-		endpoints, found, err := external.GetExternalEtcdEndpoints(externalEtcd)
-		if err != nil {
-			return ctrl.Result{}, errors.Wrapf(err, "failed to get endpoint field from %v", externalEtcd.GetName())
-		}
-		if !found {
-			log.Info("Etcd endpoints not available")
-			return ctrl.Result{Requeue: true}, nil
-		}
-		currentEtcdEndpoints := strings.Split(endpoints, ",")
-		sort.Strings(currentEtcdEndpoints)
-		currentKCPEndpoints := kcp.Spec.KubeadmConfigSpec.ClusterConfiguration.Etcd.External.Endpoints
-		if !reflect.DeepEqual(currentEtcdEndpoints, currentKCPEndpoints) {
-			kcp.Spec.KubeadmConfigSpec.ClusterConfiguration.Etcd.External.Endpoints = currentEtcdEndpoints
-			if err := patchHelper.Patch(ctx, kcp); err != nil {
-				log.Error(err, "Failed to patch KubeadmControlPlane to update external etcd endpoints")
-				return ctrl.Result{}, err
-			}
+		if !managedEtcdResult.IsZero() {
+			return managedEtcdResult, nil
 		}
 	}
 
@@ -334,6 +319,80 @@ func (r *KubeadmControlPlaneReconciler) initControlPlaneScope(ctx context.Contex
 	return controlPlane, false, nil
 }
 
+func (r *KubeadmControlPlaneReconciler) updateManagedExternalEtcdEndpoints(
+	ctx context.Context, log logr.Logger, patchHelper *patch.Helper, cluster *clusterv1.Cluster, kcp *controlplanev1.KubeadmControlPlane,
+) (ctrl.Result, error) {
+	if kcp.Spec.KubeadmConfigSpec.ClusterConfiguration == nil || kcp.Spec.KubeadmConfigSpec.ClusterConfiguration.Etcd.External == nil {
+		return ctrl.Result{}, errors.New("invalid kcp, external etcd not configured for cluster with managed external etcd")
+	}
+
+	etcdRef := cluster.Spec.ManagedExternalEtcdRef
+	externalEtcd, err := external.Get(ctx, r.Client, etcdRef, cluster.Namespace)
+	if err != nil {
+		return ctrl.Result{}, err
+	}
+
+	externalEtcdReady, err := external.IsReady(externalEtcd)
+	if err != nil {
+		return ctrl.Result{}, err
+	}
+
+	if !externalEtcdReady {
+		log.Info("Managed external etcd is not ready yet, requeueing")
+		return ctrl.Result{RequeueAfter: 1 * time.Minute}, nil
+	}
+
+	endpoints, found, err := external.GetExternalEtcdEndpoints(externalEtcd)
+	if err != nil {
+		return ctrl.Result{}, errors.Wrapf(err, "failed to get endpoint field from %v", externalEtcd.GetName())
+	}
+	currentEtcdEndpoints := strings.Split(endpoints, ",")
+
+	if !found || areEndpointsEmpty(currentEtcdEndpoints) {
+		log.Info("Managed external etcd endpoints not available, requeueing")
+		return ctrl.Result{RequeueAfter: 1 * time.Minute}, nil
+	}
+
+	sort.Strings(currentEtcdEndpoints)
+	currentKCPEndpoints := kcp.Spec.KubeadmConfigSpec.ClusterConfiguration.Etcd.External.Endpoints
+	if !reflect.DeepEqual(currentEtcdEndpoints, currentKCPEndpoints) {
+		/* During upgrade, KCP spec's endpoints will again be an empty list, and will get populated by the cluster controller once the
+		external etcd controller has set them. If the KCP controller proceeds without checking whether the etcd cluster is undergoing upgrade,
+		there is a chance it will get the current un-updated endpoints from the etcd cluster object, and those would end up being endpoints of the
+		etcd members that will get deleted during upgrade. Hence the controller checks and stalls if the etcd cluster is undergoing upgrade and proceeds
+		only after the etcd upgrade is completed as that guarantees that the KCP has latest set of endpoints.
+		*/
+		etcdUpgradeInProgress, err := external.IsExternalEtcdUpgrading(externalEtcd)
+		if err != nil {
+			return ctrl.Result{}, err
+		}
+		if etcdUpgradeInProgress {
+			log.Info("Etcd undergoing upgrade, marking etcd endpoints available condition as false, since new endpoints will be available only after etcd upgrade")
+			if conditions.IsTrue(kcp, controlplanev1.ExternalEtcdEndpointsAvailable) || conditions.IsUnknown(kcp, controlplanev1.ExternalEtcdEndpointsAvailable) {
+				conditions.MarkFalse(kcp, controlplanev1.ExternalEtcdEndpointsAvailable, controlplanev1.ExternalEtcdUndergoingUpgrade, clusterv1.ConditionSeverityInfo, "")
+				if err := patchKubeadmControlPlane(ctx, patchHelper, kcp); err != nil {
+					return ctrl.Result{}, err
+				}
+			}
+			return ctrl.Result{RequeueAfter: 1 * time.Minute}, nil
+		}
+		kcp.Spec.KubeadmConfigSpec.ClusterConfiguration.Etcd.External.Endpoints = currentEtcdEndpoints
+		if err := patchHelper.Patch(ctx, kcp); err != nil {
+			log.Error(err, "Failed to patch KubeadmControlPlane to update external etcd endpoints")
+			return ctrl.Result{}, err
+		}
+	}
+	if conditions.IsFalse(kcp, controlplanev1.ExternalEtcdEndpointsAvailable) {
+		conditions.MarkTrue(kcp, controlplanev1.ExternalEtcdEndpointsAvailable)
+	}
+
+	return ctrl.Result{}, nil
+}
+
+func areEndpointsEmpty(endpoints []string) bool {
+	return len(endpoints) == 0 || len(endpoints) == 1 && endpoints[0] == ""
+}
+
 func patchKubeadmControlPlane(ctx context.Context, patchHelper *patch.Helper, kcp *controlplanev1.KubeadmControlPlane) error {
 	// Always update the readyCondition by summarizing the state of other conditions.
 	conditions.SetSummary(kcp,
@@ -441,6 +500,25 @@ func (r *KubeadmControlPlaneReconciler) reconcile(ctx context.Context, controlPl
 		// NOTE: we are checking the condition already exists in order to avoid to set this condition at the first
 		// reconciliation/before a rolling upgrade actually starts.
 		if conditions.Has(controlPlane.KCP, controlplanev1.MachinesSpecUpToDateCondition) {
+			if conditions.IsFalse(controlPlane.KCP, controlplanev1.MachinesSpecUpToDateCondition) {
+				/* Once KCP upgrade has completed, the controller will annotate the external etcd object to indicate that the older KCP machines
+				are no longer part of the cluster, and so any older out-of-date etcd members and machines can be deleted
+				*/
+				if controlPlane.Cluster.Spec.ManagedExternalEtcdRef != nil {
+					etcdRef := controlPlane.Cluster.Spec.ManagedExternalEtcdRef
+					externalEtcd, err := external.Get(ctx, r.Client, etcdRef, controlPlane.Cluster.Namespace)
+					if err != nil {
+						return ctrl.Result{}, err
+					}
+					log.Info("Adding upgrade complete annotation on etcdadmCluster")
+					if err := external.SetKCPUpdateCompleteAnnotationOnEtcdadmCluster(externalEtcd); err != nil {
+						return ctrl.Result{}, err
+					}
+					if err := r.Client.Update(ctx, externalEtcd); err != nil {
+						return ctrl.Result{}, err
+					}
+				}
+			}
 			conditions.MarkTrue(controlPlane.KCP, controlplanev1.MachinesSpecUpToDateCondition)
 		}
 	}
diff --git a/controlplane/kubeadm/internal/controllers/controller_test.go b/controlplane/kubeadm/internal/controllers/controller_test.go
index 988ce54be..d352f7863 100644
--- a/controlplane/kubeadm/internal/controllers/controller_test.go
+++ b/controlplane/kubeadm/internal/controllers/controller_test.go
@@ -24,6 +24,7 @@ import (
 	"crypto/x509/pkix"
 	"fmt"
 	"math/big"
+	"strings"
 	"sync"
 	"testing"
 	"time"
@@ -58,6 +59,7 @@ import (
 	"sigs.k8s.io/cluster-api/internal/util/ssa"
 	"sigs.k8s.io/cluster-api/internal/webhooks"
 	"sigs.k8s.io/cluster-api/util"
+	"sigs.k8s.io/cluster-api/util/annotations"
 	"sigs.k8s.io/cluster-api/util/certs"
 	"sigs.k8s.io/cluster-api/util/collections"
 	"sigs.k8s.io/cluster-api/util/conditions"
@@ -84,7 +86,8 @@ func TestClusterToKubeadmControlPlane(t *testing.T) {
 		{
 			NamespacedName: client.ObjectKey{
 				Namespace: cluster.Spec.ControlPlaneRef.Namespace,
-				Name:      cluster.Spec.ControlPlaneRef.Name},
+				Name:      cluster.Spec.ControlPlaneRef.Name,
+			},
 		},
 	}
 
@@ -2285,6 +2288,210 @@ func TestKubeadmControlPlaneReconciler_reconcileDelete(t *testing.T) {
 	})
 }
 
+func TestKubeadmControlPlaneReconciler_updateManagedExternalEtcdEndpoints(t *testing.T) {
+	setup := func() (*clusterv1.Cluster, *controlplanev1.KubeadmControlPlane, *unstructured.Unstructured) {
+		ns := "my-ns"
+		endpoints := []string{"1.1.1.1", "2.2.2.2", "0.0.0.0"}
+		managedEtcd := builder.Etcd(ns, "test-7-my-etcd").Build()
+		unstructured.SetNestedField(managedEtcd.Object, true, "status", "ready")
+		unstructured.SetNestedField(managedEtcd.Object, strings.Join(endpoints, ","), "status", "endpoints")
+		cluster, kcp, _ := createClusterWithControlPlane(ns)
+		cluster.Spec.ManagedExternalEtcdRef = external.GetObjectReference(managedEtcd)
+		kcp.Spec.KubeadmConfigSpec.ClusterConfiguration = &bootstrapv1.ClusterConfiguration{
+			Etcd: bootstrapv1.Etcd{External: &bootstrapv1.ExternalEtcd{}},
+		}
+
+		return cluster, kcp, managedEtcd
+	}
+	t.Run("should update the endpoints in the kcp", func(t *testing.T) {
+		g := NewWithT(t)
+		cluster, kcp, managedEtcd := setup()
+		conditions.MarkFalse(kcp, controlplanev1.ExternalEtcdEndpointsAvailable, "", "", "")
+
+		fClient := newFakeClient(
+			builder.GenericEtcdCRD.DeepCopy(),
+			managedEtcd.DeepCopy(),
+			cluster.DeepCopy(),
+			kcp.DeepCopy(),
+		)
+
+		r := &KubeadmControlPlaneReconciler{
+			Client: fClient,
+			managementCluster: &fakeManagementCluster{
+				Management: &internal.Management{Client: fClient},
+				Workload:   fakeWorkloadCluster{},
+			},
+			recorder: record.NewFakeRecorder(32),
+		}
+
+		result, err := r.Reconcile(
+			ctx,
+			ctrl.Request{client.ObjectKeyFromObject(kcp)},
+		)
+		g.Expect(result).To(Equal(ctrl.Result{}))
+		g.Expect(err).NotTo(HaveOccurred())
+		g.Eventually(func(g Gomega) {
+			cp := &controlplanev1.KubeadmControlPlane{}
+			g.Expect(fClient.Get(ctx, client.ObjectKeyFromObject(kcp), cp)).To(Succeed())
+			g.Expect(
+				cp.Spec.KubeadmConfigSpec.ClusterConfiguration.Etcd.External.Endpoints,
+			).To(Equal([]string{"0.0.0.0", "1.1.1.1", "2.2.2.2"}))
+			conditions.IsTrue(kcp, controlplanev1.ExternalEtcdEndpointsAvailable)
+		}, 5*time.Second).Should(Succeed())
+	})
+
+	t.Run("should requeue and not update kcp when endpoints in external etcd are not set", func(t *testing.T) {
+		g := NewWithT(t)
+		cluster, kcp, managedEtcd := setup()
+		unstructured.RemoveNestedField(managedEtcd.Object, "status", "endpoints")
+		kcp.Spec.KubeadmConfigSpec.ClusterConfiguration.Etcd.External.Endpoints = []string{"0.0.0.0", "1.1.1.1", "3.3.3.3"}
+
+		fClient := newFakeClient(
+			builder.GenericEtcdCRD.DeepCopy(),
+			managedEtcd.DeepCopy(),
+			cluster.DeepCopy(),
+			kcp.DeepCopy(),
+		)
+
+		r := &KubeadmControlPlaneReconciler{
+			Client: fClient,
+			managementCluster: &fakeManagementCluster{
+				Management: &internal.Management{Client: fClient},
+				Workload:   fakeWorkloadCluster{},
+			},
+			recorder: record.NewFakeRecorder(32),
+		}
+
+		result, err := r.Reconcile(
+			ctx,
+			ctrl.Request{client.ObjectKeyFromObject(kcp)},
+		)
+		g.Expect(result).To(Equal(ctrl.Result{RequeueAfter: 1 * time.Minute}))
+		g.Expect(err).NotTo(HaveOccurred())
+		g.Eventually(func(g Gomega) {
+			cp := &controlplanev1.KubeadmControlPlane{}
+			g.Expect(fClient.Get(ctx, client.ObjectKeyFromObject(kcp), cp)).To(Succeed())
+			g.Expect(
+				cp.Spec.KubeadmConfigSpec.ClusterConfiguration.Etcd.External.Endpoints,
+			).To(Equal([]string{"0.0.0.0", "1.1.1.1", "3.3.3.3"}))
+		}, 5*time.Second).Should(Succeed())
+	})
+
+	t.Run("should requeue and not update kcp when endpoints in external etcd are empty", func(t *testing.T) {
+		g := NewWithT(t)
+		cluster, kcp, managedEtcd := setup()
+		unstructured.SetNestedField(managedEtcd.Object, "", "status", "endpoints")
+		kcp.Spec.KubeadmConfigSpec.ClusterConfiguration.Etcd.External.Endpoints = []string{"0.0.0.0", "1.1.1.1", "3.3.3.3"}
+
+		fClient := newFakeClient(
+			builder.GenericEtcdCRD.DeepCopy(),
+			managedEtcd.DeepCopy(),
+			cluster.DeepCopy(),
+			kcp.DeepCopy(),
+		)
+
+		r := &KubeadmControlPlaneReconciler{
+			Client: fClient,
+			managementCluster: &fakeManagementCluster{
+				Management: &internal.Management{Client: fClient},
+				Workload:   fakeWorkloadCluster{},
+			},
+			recorder: record.NewFakeRecorder(32),
+		}
+
+		result, err := r.Reconcile(
+			ctx,
+			ctrl.Request{client.ObjectKeyFromObject(kcp)},
+		)
+		g.Expect(result).To(Equal(ctrl.Result{RequeueAfter: 1 * time.Minute}))
+		g.Expect(err).NotTo(HaveOccurred())
+		g.Eventually(func(g Gomega) {
+			cp := &controlplanev1.KubeadmControlPlane{}
+			g.Expect(fClient.Get(ctx, client.ObjectKeyFromObject(kcp), cp)).To(Succeed())
+			g.Expect(
+				cp.Spec.KubeadmConfigSpec.ClusterConfiguration.Etcd.External.Endpoints,
+			).To(Equal([]string{"0.0.0.0", "1.1.1.1", "3.3.3.3"}))
+		}, 5*time.Second).Should(Succeed())
+	})
+
+	t.Run("should requeue and not update kcp when endpoints in external etcd is not ready", func(t *testing.T) {
+		g := NewWithT(t)
+		cluster, kcp, managedEtcd := setup()
+		unstructured.SetNestedField(managedEtcd.Object, "0.0.0.0", "status", "endpoints")
+		unstructured.SetNestedField(managedEtcd.Object, false, "status", "ready")
+		kcp.Spec.KubeadmConfigSpec.ClusterConfiguration.Etcd.External.Endpoints = []string{"0.0.0.0", "1.1.1.1", "3.3.3.3"}
+
+		fClient := newFakeClient(
+			builder.GenericEtcdCRD.DeepCopy(),
+			managedEtcd.DeepCopy(),
+			cluster.DeepCopy(),
+			kcp.DeepCopy(),
+		)
+
+		r := &KubeadmControlPlaneReconciler{
+			Client: fClient,
+			managementCluster: &fakeManagementCluster{
+				Management: &internal.Management{Client: fClient},
+				Workload:   fakeWorkloadCluster{},
+			},
+			recorder: record.NewFakeRecorder(32),
+		}
+
+		result, err := r.Reconcile(
+			ctx,
+			ctrl.Request{client.ObjectKeyFromObject(kcp)},
+		)
+		g.Expect(result).To(Equal(ctrl.Result{RequeueAfter: 1 * time.Minute}))
+		g.Expect(err).NotTo(HaveOccurred())
+		g.Eventually(func(g Gomega) {
+			cp := &controlplanev1.KubeadmControlPlane{}
+			g.Expect(fClient.Get(ctx, client.ObjectKeyFromObject(kcp), cp)).To(Succeed())
+			g.Expect(
+				cp.Spec.KubeadmConfigSpec.ClusterConfiguration.Etcd.External.Endpoints,
+			).To(Equal([]string{"0.0.0.0", "1.1.1.1", "3.3.3.3"}))
+		}, 5*time.Second).Should(Succeed())
+	})
+
+	t.Run("should requeue and not update kcp when etcd is ongoing an upgrade in external etcd is going through an upgrade", func(t *testing.T) {
+		g := NewWithT(t)
+		cluster, kcp, managedEtcd := setup()
+		unstructured.SetNestedField(managedEtcd.Object, "0.0.0.0", "status", "endpoints")
+		annotations.AddAnnotations(managedEtcd, map[string]string{"etcdcluster.cluster.x-k8s.io/upgrading": "true"})
+		kcp.Spec.KubeadmConfigSpec.ClusterConfiguration.Etcd.External.Endpoints = []string{"0.0.0.0", "1.1.1.1", "3.3.3.3"}
+
+		fClient := newFakeClient(
+			builder.GenericEtcdCRD.DeepCopy(),
+			managedEtcd.DeepCopy(),
+			cluster.DeepCopy(),
+			kcp.DeepCopy(),
+		)
+
+		r := &KubeadmControlPlaneReconciler{
+			Client: fClient,
+			managementCluster: &fakeManagementCluster{
+				Management: &internal.Management{Client: fClient},
+				Workload:   fakeWorkloadCluster{},
+			},
+			recorder: record.NewFakeRecorder(32),
+		}
+
+		result, err := r.Reconcile(
+			ctx,
+			ctrl.Request{client.ObjectKeyFromObject(kcp)},
+		)
+		g.Expect(result).To(Equal(ctrl.Result{RequeueAfter: 1 * time.Minute}))
+		g.Expect(err).NotTo(HaveOccurred())
+		g.Eventually(func(g Gomega) {
+			cp := &controlplanev1.KubeadmControlPlane{}
+			g.Expect(fClient.Get(ctx, client.ObjectKeyFromObject(kcp), cp)).To(Succeed())
+			g.Expect(
+				cp.Spec.KubeadmConfigSpec.ClusterConfiguration.Etcd.External.Endpoints,
+			).To(Equal([]string{"0.0.0.0", "1.1.1.1", "3.3.3.3"}))
+			conditions.IsFalse(cp, controlplanev1.ExternalEtcdEndpointsAvailable)
+		}, 5*time.Second).Should(Succeed())
+	})
+}
+
 // test utils.
 
 func newFakeClient(initObjs ...client.Object) client.Client {
diff --git a/controlplane/kubeadm/internal/controllers/upgrade.go b/controlplane/kubeadm/internal/controllers/upgrade.go
index 6abf13694..647d87f6a 100644
--- a/controlplane/kubeadm/internal/controllers/upgrade.go
+++ b/controlplane/kubeadm/internal/controllers/upgrade.go
@@ -104,6 +104,12 @@ func (r *KubeadmControlPlaneReconciler) upgradeControlPlane(
 		}
 	}
 
+	if controlPlane.KCP.Spec.KubeadmConfigSpec.ClusterConfiguration != nil && controlPlane.KCP.Spec.KubeadmConfigSpec.ClusterConfiguration.Etcd.External != nil {
+		if err := workloadCluster.UpdateExternalEtcdEndpointsInKubeadmConfigMap(ctx, controlPlane.KCP.Spec.KubeadmConfigSpec.ClusterConfiguration.Etcd.External.Endpoints, parsedVersion); err != nil {
+			return ctrl.Result{}, errors.Wrap(err, "failed to update the external etcd endpoints in the kubeadm config map")
+		}
+	}
+
 	if controlPlane.KCP.Spec.KubeadmConfigSpec.ClusterConfiguration != nil {
 		if err := workloadCluster.UpdateAPIServerInKubeadmConfigMap(ctx, controlPlane.KCP.Spec.KubeadmConfigSpec.ClusterConfiguration.APIServer, parsedVersion); err != nil {
 			return ctrl.Result{}, errors.Wrap(err, "failed to update api server in the kubeadm config map")
diff --git a/controlplane/kubeadm/internal/workload_cluster.go b/controlplane/kubeadm/internal/workload_cluster.go
index d4c41eb89..cf6675f2f 100644
--- a/controlplane/kubeadm/internal/workload_cluster.go
+++ b/controlplane/kubeadm/internal/workload_cluster.go
@@ -109,6 +109,7 @@ type WorkloadCluster interface {
 	UpdateImageRepositoryInKubeadmConfigMap(ctx context.Context, imageRepository string, version semver.Version) error
 	UpdateEtcdVersionInKubeadmConfigMap(ctx context.Context, imageRepository, imageTag string, version semver.Version) error
 	UpdateEtcdExtraArgsInKubeadmConfigMap(ctx context.Context, extraArgs map[string]string, version semver.Version) error
+	UpdateExternalEtcdEndpointsInKubeadmConfigMap(ctx context.Context, endpoints []string, version semver.Version) error
 	UpdateAPIServerInKubeadmConfigMap(ctx context.Context, apiServer bootstrapv1.APIServer, version semver.Version) error
 	UpdateControllerManagerInKubeadmConfigMap(ctx context.Context, controllerManager bootstrapv1.ControlPlaneComponent, version semver.Version) error
 	UpdateSchedulerInKubeadmConfigMap(ctx context.Context, scheduler bootstrapv1.ControlPlaneComponent, version semver.Version) error
diff --git a/controlplane/kubeadm/internal/workload_cluster_etcd.go b/controlplane/kubeadm/internal/workload_cluster_etcd.go
index bb4c4d417..d2850c14e 100644
--- a/controlplane/kubeadm/internal/workload_cluster_etcd.go
+++ b/controlplane/kubeadm/internal/workload_cluster_etcd.go
@@ -111,6 +111,14 @@ func (w *Workload) UpdateEtcdExtraArgsInKubeadmConfigMap(ctx context.Context, ex
 	}, version)
 }
 
+func (w *Workload) UpdateExternalEtcdEndpointsInKubeadmConfigMap(ctx context.Context, endpoints []string, version semver.Version) error {
+	return w.updateClusterConfiguration(ctx, func(c *bootstrapv1.ClusterConfiguration) {
+		if c.Etcd.External != nil {
+			c.Etcd.External.Endpoints = endpoints
+		}
+	}, version)
+}
+
 // RemoveEtcdMemberForMachine removes the etcd member from the target cluster's etcd cluster.
 // Removing the last remaining member of the cluster is not supported.
 func (w *Workload) RemoveEtcdMemberForMachine(ctx context.Context, machine *clusterv1.Machine) error {
diff --git a/internal/controllers/cluster/cluster_controller.go b/internal/controllers/cluster/cluster_controller.go
index 269007490..b41895e51 100644
--- a/internal/controllers/cluster/cluster_controller.go
+++ b/internal/controllers/cluster/cluster_controller.go
@@ -206,10 +206,10 @@ func (r *Reconciler) reconcile(ctx context.Context, cluster *clusterv1.Cluster)
 
 	phases := []func(context.Context, *clusterv1.Cluster) (ctrl.Result, error){
 		r.reconcileInfrastructure,
+		r.reconcileEtcdCluster,
 		r.reconcileControlPlane,
 		r.reconcileKubeconfig,
 		r.reconcileControlPlaneInitialized,
-		r.reconcileEtcdCluster,
 	}
 
 	res := ctrl.Result{}
diff --git a/internal/controllers/cluster/cluster_controller_phases.go b/internal/controllers/cluster/cluster_controller_phases.go
index 590967dd6..cb6e878e8 100644
--- a/internal/controllers/cluster/cluster_controller_phases.go
+++ b/internal/controllers/cluster/cluster_controller_phases.go
@@ -19,13 +19,13 @@ package cluster
 import (
 	"context"
 	"fmt"
-	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
-	"sigs.k8s.io/controller-runtime/pkg/client"
 	"time"
 
+	"github.com/go-logr/logr"
 	"github.com/pkg/errors"
 	corev1 "k8s.io/api/core/v1"
 	apierrors "k8s.io/apimachinery/pkg/api/errors"
+	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
 	"k8s.io/utils/pointer"
 	ctrl "sigs.k8s.io/controller-runtime"
 	"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
@@ -223,37 +223,13 @@ func (r *Reconciler) reconcileControlPlane(ctx context.Context, cluster *cluster
 	if cluster.Spec.ControlPlaneRef == nil {
 		return ctrl.Result{}, nil
 	}
-	log := ctrl.LoggerFrom(ctx)
+
 	if cluster.Spec.ManagedExternalEtcdRef != nil {
-		// check if the referenced etcd cluster is ready or not
-		etcdRef := cluster.Spec.ManagedExternalEtcdRef
-		externalEtcd, err := external.Get(ctx, r.Client, etcdRef, cluster.Namespace)
-		if err != nil {
-			if apierrors.IsNotFound(errors.Cause(err)) {
-				log.Info("Could not find external object for cluster, requeuing", "refGroupVersionKind", etcdRef.GroupVersionKind(), "refName", etcdRef.Name)
-				return ctrl.Result{RequeueAfter: 30 * time.Second}, nil
-			}
+		log := ctrl.LoggerFrom(ctx)
+		if result, err := r.handlePauseControlPlaneWithExternalManagedEtcd(ctx, log, cluster); err != nil {
 			return ctrl.Result{}, err
-		}
-		externalEtcdReady, err := external.IsReady(externalEtcd)
-		if err != nil {
-			return ctrl.Result{}, err
-		}
-		if !externalEtcdReady {
-			// External Etcd Cluster has not been created, pause control plane provisioning by setting the paused annotation on the Control plane object
-			controlPlane, err := external.Get(ctx, r.Client, cluster.Spec.ControlPlaneRef, cluster.Namespace)
-			if err != nil {
-				if apierrors.IsNotFound(errors.Cause(err)) {
-					log.Info("Could not find control plane for cluster, requeuing", "refGroupVersionKind", cluster.Spec.ControlPlaneRef.GroupVersionKind(), "refName", cluster.Spec.ControlPlaneRef.Name)
-					return ctrl.Result{RequeueAfter: 30 * time.Second}, nil
-				}
-				return ctrl.Result{}, err
-			}
-			annotations.AddAnnotations(controlPlane, map[string]string{clusterv1.PausedAnnotation: "true"})
-			if err := r.Client.Update(ctx, controlPlane, &client.UpdateOptions{}); err != nil {
-				log.Error(err, "error pausing control plane")
-				return ctrl.Result{Requeue: true}, err
-			}
+		} else if !result.IsZero() {
+			return result, nil
 		}
 	}
 
@@ -312,6 +288,55 @@ func (r *Reconciler) reconcileControlPlane(ctx context.Context, cluster *cluster
 	return ctrl.Result{}, nil
 }
 
+// handlePauseControlPlaneWithExternalManagedEtcd pauses or unpauses the control plane through the pause
+// annotation based on the readiness of the external managed etcd (not-ready -> pause or ready -> unpause)
+func (r *Reconciler) handlePauseControlPlaneWithExternalManagedEtcd(ctx context.Context, log logr.Logger, cluster *clusterv1.Cluster) (ctrl.Result, error) {
+	controlPlane, err := external.Get(ctx, r.Client, cluster.Spec.ControlPlaneRef, cluster.Namespace)
+	if apierrors.IsNotFound(errors.Cause(err)) {
+		log.Info("Could not find control plane object for cluster, requeuing", "refGroupVersionKind",
+			cluster.Spec.ControlPlaneRef.GroupVersionKind(), "refName", cluster.Spec.ControlPlaneRef.Name,
+		)
+		return ctrl.Result{RequeueAfter: 30 * time.Second}, nil
+	}
+	if err != nil {
+		return ctrl.Result{}, err
+	}
+
+	// If user has opt-out from the pause/unpause functionality, just exit
+	if annotations.HasAnnotation(controlPlane, clusterv1.SkipControlPlanePauseManagedEtcdAnnotation) {
+		return ctrl.Result{}, nil
+	}
+
+	etcdRef := cluster.Spec.ManagedExternalEtcdRef
+	externalEtcd, err := external.Get(ctx, r.Client, etcdRef, cluster.Namespace)
+	if err != nil {
+		if apierrors.IsNotFound(errors.Cause(err)) {
+			log.Info("Could not find external object for cluster, requeuing", "refGroupVersionKind", etcdRef.GroupVersionKind(), "refName", etcdRef.Name)
+			return ctrl.Result{RequeueAfter: 30 * time.Second}, nil
+		}
+		return ctrl.Result{}, err
+	}
+
+	externalEtcdReady, err := external.IsReady(externalEtcd)
+	if err != nil {
+		return ctrl.Result{}, err
+	}
+
+	if externalEtcdReady && annotations.HasPaused(controlPlane) {
+		unstructured.RemoveNestedField(controlPlane.Object, "metadata", "annotations", clusterv1.PausedAnnotation)
+		if err := r.Client.Update(ctx, controlPlane); err != nil {
+			return ctrl.Result{Requeue: true}, errors.Wrap(err, "resuming control plane reconcile")
+		}
+	} else if !externalEtcdReady && !annotations.HasPaused(controlPlane) {
+		annotations.AddAnnotations(controlPlane, map[string]string{clusterv1.PausedAnnotation: "true"})
+		if err := r.Client.Update(ctx, controlPlane); err != nil {
+			return ctrl.Result{}, errors.Wrap(err, "pausing control plane reconcile")
+		}
+	}
+
+	return ctrl.Result{}, nil
+}
+
 func (r *Reconciler) reconcileEtcdCluster(ctx context.Context, cluster *clusterv1.Cluster) (ctrl.Result, error) {
 	log := ctrl.LoggerFrom(ctx)
 
@@ -345,23 +370,6 @@ func (r *Reconciler) reconcileEtcdCluster(ctx context.Context, cluster *clusterv
 	}
 	cluster.Status.ManagedExternalEtcdReady = ready
 
-	if ready {
-		// resume control plane
-		controlPlane, err := external.Get(ctx, r.Client, cluster.Spec.ControlPlaneRef, cluster.Namespace)
-		if err != nil {
-			if apierrors.IsNotFound(errors.Cause(err)) {
-				log.Info("Could not find control plane for cluster, requeuing", "refGroupVersionKind", cluster.Spec.ControlPlaneRef.GroupVersionKind(), "refName", cluster.Spec.ControlPlaneRef.Name)
-				return ctrl.Result{RequeueAfter: 30 * time.Second}, nil
-			}
-			return ctrl.Result{}, err
-		}
-		unstructured.RemoveNestedField(controlPlane.Object, "metadata", "annotations", clusterv1.PausedAnnotation)
-		if err := r.Client.Update(ctx, controlPlane, &client.UpdateOptions{}); err != nil {
-			log.Error(err, "error resuming control plane")
-			return ctrl.Result{Requeue: true}, err
-		}
-	}
-
 	// Report a summary of current status of the etcd cluster object defined for this cluster.
 	conditions.SetMirror(cluster, clusterv1.ManagedExternalEtcdClusterReadyCondition,
 		conditions.UnstructuredGetter(etcdPlaneConfig),
diff --git a/internal/controllers/cluster/cluster_controller_test.go b/internal/controllers/cluster/cluster_controller_test.go
index bce99a762..d6afe09dc 100644
--- a/internal/controllers/cluster/cluster_controller_test.go
+++ b/internal/controllers/cluster/cluster_controller_test.go
@@ -18,16 +18,21 @@ package cluster
 
 import (
 	"testing"
+	"time"
 
 	. "github.com/onsi/gomega"
 	corev1 "k8s.io/api/core/v1"
 	apierrors "k8s.io/apimachinery/pkg/api/errors"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
+	"k8s.io/client-go/tools/record"
 	utilfeature "k8s.io/component-base/featuregate/testing"
 	"k8s.io/utils/pointer"
 	ctrl "sigs.k8s.io/controller-runtime"
 	"sigs.k8s.io/controller-runtime/pkg/client"
 	"sigs.k8s.io/controller-runtime/pkg/client/fake"
+	"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
+	"sigs.k8s.io/controller-runtime/pkg/reconcile"
 
 	clusterv1 "sigs.k8s.io/cluster-api/api/v1beta1"
 	expv1 "sigs.k8s.io/cluster-api/exp/api/v1beta1"
@@ -35,6 +40,7 @@ import (
 	"sigs.k8s.io/cluster-api/feature"
 	"sigs.k8s.io/cluster-api/internal/test/builder"
 	"sigs.k8s.io/cluster-api/util"
+	"sigs.k8s.io/cluster-api/util/annotations"
 	"sigs.k8s.io/cluster-api/util/conditions"
 	"sigs.k8s.io/cluster-api/util/patch"
 )
@@ -944,3 +950,254 @@ func TestReconcileControlPlaneInitializedControlPlaneRef(t *testing.T) {
 	g.Expect(err).ToNot(HaveOccurred())
 	g.Expect(conditions.Has(c, clusterv1.ControlPlaneInitializedCondition)).To(BeFalse())
 }
+
+func TestReconcileWithManagedEtcd(t *testing.T) {
+	t.Run("Should pause the ControlPlane when the external etcd becomes NotReady", func(t *testing.T) {
+		g := NewWithT(t)
+		ns := "my-ns"
+
+		managedEtcd := builder.Etcd(ns, "test-7-my-etcd").Build()
+		controlPlane := builder.TestControlPlane(ns, "test-7-my-cp").Build()
+		cluster := builder.Cluster(ns, "test-7-my-cluster").
+			WithControlPlane(controlPlane).
+			WithManagedEtcd(managedEtcd).
+			Build()
+		controllerutil.AddFinalizer(cluster, clusterv1.ClusterFinalizer)
+
+		c := fake.NewClientBuilder().
+			WithObjects(
+				builder.GenericEtcdCRD.DeepCopy(),
+				builder.TestControlPlaneCRD.DeepCopy(),
+				managedEtcd,
+				controlPlane,
+				cluster,
+			).Build()
+
+		r := &Reconciler{
+			Client:   c,
+			recorder: record.NewFakeRecorder(32),
+		}
+
+		result, err := r.Reconcile(
+			ctx,
+			reconcile.Request{NamespacedName: client.ObjectKeyFromObject(cluster)},
+		)
+		g.Expect(result).To(BeZero())
+		g.Expect(err).NotTo(HaveOccurred())
+
+		g.Eventually(func(g Gomega) {
+			cp := builder.TestControlPlane("", "").Build()
+			g.Expect(c.Get(ctx, client.ObjectKeyFromObject(controlPlane), cp)).To(Succeed())
+			g.Expect(annotations.HasPaused(cp)).To(BeTrue())
+		}, 5*time.Second).Should(Succeed())
+	})
+
+	t.Run("Should unpause the ControlPlane when the external etcd becomes Ready", func(t *testing.T) {
+		g := NewWithT(t)
+		ns := "my-ns"
+
+		managedEtcd := builder.Etcd(ns, "test-7-my-etcd").Build()
+		unstructured.SetNestedField(managedEtcd.Object, true, "status", "ready")
+		controlPlane := builder.TestControlPlane(ns, "test-7-my-cp").Build()
+		annotations.AddAnnotations(controlPlane, map[string]string{clusterv1.PausedAnnotation: "true"})
+		g.Expect(annotations.HasPaused(controlPlane)).To(BeTrue())
+		cluster := builder.Cluster(ns, "test-7-my-cluster").
+			WithControlPlane(controlPlane).
+			WithManagedEtcd(managedEtcd).
+			Build()
+		controllerutil.AddFinalizer(cluster, clusterv1.ClusterFinalizer)
+
+		c := fake.NewClientBuilder().
+			WithObjects(
+				builder.GenericEtcdCRD.DeepCopy(),
+				builder.TestControlPlaneCRD.DeepCopy(),
+				managedEtcd,
+				controlPlane,
+				cluster,
+			).Build()
+
+		r := &Reconciler{
+			Client:   c,
+			recorder: record.NewFakeRecorder(32),
+		}
+
+		result, err := r.Reconcile(
+			ctx,
+			reconcile.Request{NamespacedName: client.ObjectKeyFromObject(cluster)},
+		)
+		g.Expect(result).To(BeZero())
+		g.Expect(err).NotTo(HaveOccurred())
+
+		g.Eventually(func(g Gomega) {
+			cp := builder.TestControlPlane("", "").Build()
+			g.Expect(c.Get(ctx, client.ObjectKeyFromObject(controlPlane), cp)).To(Succeed())
+			g.Expect(annotations.HasPaused(cp)).To(BeFalse())
+		}, 5*time.Second).Should(Succeed())
+	})
+
+	t.Run("Should keep the ControlPlane unpaused when the external etcd is Ready", func(t *testing.T) {
+		g := NewWithT(t)
+		ns := "my-ns"
+
+		managedEtcd := builder.Etcd(ns, "test-7-my-etcd").Build()
+		unstructured.SetNestedField(managedEtcd.Object, true, "status", "ready")
+		controlPlane := builder.TestControlPlane(ns, "test-7-my-cp").Build()
+		cluster := builder.Cluster(ns, "test-7-my-cluster").
+			WithControlPlane(controlPlane).
+			WithManagedEtcd(managedEtcd).
+			Build()
+		controllerutil.AddFinalizer(cluster, clusterv1.ClusterFinalizer)
+
+		c := fake.NewClientBuilder().
+			WithObjects(
+				builder.GenericEtcdCRD.DeepCopy(),
+				builder.TestControlPlaneCRD.DeepCopy(),
+				managedEtcd,
+				controlPlane,
+				cluster,
+			).Build()
+
+		r := &Reconciler{
+			Client:   c,
+			recorder: record.NewFakeRecorder(32),
+		}
+
+		result, err := r.Reconcile(
+			ctx,
+			reconcile.Request{NamespacedName: client.ObjectKeyFromObject(cluster)},
+		)
+		g.Expect(result).To(BeZero())
+		g.Expect(err).NotTo(HaveOccurred())
+
+		g.Eventually(func(g Gomega) {
+			cp := builder.TestControlPlane("", "").Build()
+			g.Expect(c.Get(ctx, client.ObjectKeyFromObject(controlPlane), cp)).To(Succeed())
+			g.Expect(annotations.HasPaused(cp)).To(BeFalse())
+		}, 5*time.Second).Should(Succeed())
+	})
+
+	t.Run("Should not pause the ControlPlane with the skip annotation when the external etcd becomes NotReady", func(t *testing.T) {
+		g := NewWithT(t)
+		ns := "my-ns"
+
+		managedEtcd := builder.Etcd(ns, "test-7-my-etcd").Build()
+		controlPlane := builder.TestControlPlane(ns, "test-7-my-cp").Build()
+		annotations.AddAnnotations(
+			controlPlane,
+			map[string]string{clusterv1.SkipControlPlanePauseManagedEtcdAnnotation: "true"},
+		)
+		cluster := builder.Cluster(ns, "test-7-my-cluster").
+			WithControlPlane(controlPlane).
+			WithManagedEtcd(managedEtcd).
+			Build()
+		controllerutil.AddFinalizer(cluster, clusterv1.ClusterFinalizer)
+
+		c := fake.NewClientBuilder().
+			WithObjects(
+				builder.GenericEtcdCRD.DeepCopy(),
+				builder.TestControlPlaneCRD.DeepCopy(),
+				managedEtcd,
+				controlPlane,
+				cluster,
+			).Build()
+
+		r := &Reconciler{
+			Client:   c,
+			recorder: record.NewFakeRecorder(32),
+		}
+
+		result, err := r.Reconcile(
+			ctx,
+			reconcile.Request{NamespacedName: client.ObjectKeyFromObject(cluster)},
+		)
+		g.Expect(result).To(BeZero())
+		g.Expect(err).NotTo(HaveOccurred())
+
+		g.Eventually(func(g Gomega) {
+			cp := builder.TestControlPlane("", "").Build()
+			g.Expect(c.Get(ctx, client.ObjectKeyFromObject(controlPlane), cp)).To(Succeed())
+			g.Expect(annotations.HasPaused(cp)).To(BeFalse())
+		}, 5*time.Second).Should(Succeed())
+	})
+
+	t.Run("Should not unpause the ControlPlane with skip annotation when the external etcd becomes Ready", func(t *testing.T) {
+		g := NewWithT(t)
+		ns := "my-ns"
+
+		managedEtcd := builder.Etcd(ns, "test-7-my-etcd").Build()
+		unstructured.SetNestedField(managedEtcd.Object, true, "status", "ready")
+		controlPlane := builder.TestControlPlane(ns, "test-7-my-cp").Build()
+		annotations.AddAnnotations(controlPlane, map[string]string{clusterv1.PausedAnnotation: "true"})
+		annotations.AddAnnotations(
+			controlPlane,
+			map[string]string{clusterv1.SkipControlPlanePauseManagedEtcdAnnotation: "true"},
+		)
+		g.Expect(annotations.HasPaused(controlPlane)).To(BeTrue())
+		cluster := builder.Cluster(ns, "test-7-my-cluster").
+			WithControlPlane(controlPlane).
+			WithManagedEtcd(managedEtcd).
+			Build()
+		controllerutil.AddFinalizer(cluster, clusterv1.ClusterFinalizer)
+
+		c := fake.NewClientBuilder().
+			WithObjects(
+				builder.GenericEtcdCRD.DeepCopy(),
+				builder.TestControlPlaneCRD.DeepCopy(),
+				managedEtcd,
+				controlPlane,
+				cluster,
+			).Build()
+
+		r := &Reconciler{
+			Client:   c,
+			recorder: record.NewFakeRecorder(32),
+		}
+
+		result, err := r.Reconcile(
+			ctx,
+			reconcile.Request{NamespacedName: client.ObjectKeyFromObject(cluster)},
+		)
+		g.Expect(result).To(BeZero())
+		g.Expect(err).NotTo(HaveOccurred())
+
+		g.Eventually(func(g Gomega) {
+			cp := builder.TestControlPlane("", "").Build()
+			g.Expect(c.Get(ctx, client.ObjectKeyFromObject(controlPlane), cp)).To(Succeed())
+			g.Expect(annotations.HasPaused(cp)).To(BeTrue())
+		}, 5*time.Second).Should(Succeed())
+	})
+
+	t.Run("Should requeue when etcd is not found", func(t *testing.T) {
+		g := NewWithT(t)
+		ns := "my-ns"
+
+		managedEtcd := builder.Etcd(ns, "test-7-my-etcd").Build()
+		unstructured.SetNestedField(managedEtcd.Object, true, "status", "ready")
+		controlPlane := builder.TestControlPlane(ns, "test-7-my-cp").Build()
+		cluster := builder.Cluster(ns, "test-7-my-cluster").
+			WithControlPlane(controlPlane).
+			WithManagedEtcd(managedEtcd).
+			Build()
+		controllerutil.AddFinalizer(cluster, clusterv1.ClusterFinalizer)
+
+		c := fake.NewClientBuilder().
+			WithObjects(
+				builder.GenericEtcdCRD.DeepCopy(),
+				builder.TestControlPlaneCRD.DeepCopy(),
+				controlPlane,
+				cluster,
+			).Build()
+
+		r := &Reconciler{
+			Client:   c,
+			recorder: record.NewFakeRecorder(32),
+		}
+
+		result, err := r.Reconcile(
+			ctx,
+			reconcile.Request{NamespacedName: client.ObjectKeyFromObject(cluster)},
+		)
+		g.Expect(result).To(Equal(ctrl.Result{RequeueAfter: 30 * time.Second}))
+		g.Expect(err).NotTo(HaveOccurred())
+	})
+}
diff --git a/internal/controllers/machine/machine_controller_noderef_test.go b/internal/controllers/machine/machine_controller_noderef_test.go
index f3586efa5..465b837a3 100644
--- a/internal/controllers/machine/machine_controller_noderef_test.go
+++ b/internal/controllers/machine/machine_controller_noderef_test.go
@@ -807,8 +807,12 @@ func TestReconcileNodeForEtcdMachines(t *testing.T) {
 		t.Run(tc.name, func(t *testing.T) {
 			g := NewWithT(t)
 			r := Reconciler{Client: env}
+			s := &scope{
+				cluster: &clusterv1.Cluster{},
+				machine: tc.machine,
+			}
 
-			g.Expect(r.reconcileNode(ctx, &clusterv1.Cluster{}, tc.machine)).To(Equal(ctrl.Result{}))
+			g.Expect(r.reconcileNode(ctx, s)).To(Equal(ctrl.Result{}))
 			g.Expect(conditions.Get(tc.machine, clusterv1.MachineNodeHealthyCondition)).To(BeNil())
 		})
 	}
diff --git a/internal/test/builder/builders.go b/internal/test/builder/builders.go
index 24e00c201..d87b0424b 100644
--- a/internal/test/builder/builders.go
+++ b/internal/test/builder/builders.go
@@ -39,6 +39,7 @@ type ClusterBuilder struct {
 	topology              *clusterv1.Topology
 	infrastructureCluster *unstructured.Unstructured
 	controlPlane          *unstructured.Unstructured
+	managedEtcd          *unstructured.Unstructured
 	network               *clusterv1.ClusterNetwork
 }
 
@@ -80,6 +81,12 @@ func (c *ClusterBuilder) WithControlPlane(t *unstructured.Unstructured) *Cluster
 	return c
 }
 
+// WithManagedEtcd adds the passed Etcd to the ClusterBuilder.
+func (c *ClusterBuilder) WithManagedEtcd(t *unstructured.Unstructured) *ClusterBuilder {
+	c.managedEtcd = t
+	return c
+}
+
 // WithTopology adds the passed Topology object to the ClusterBuilder.
 func (c *ClusterBuilder) WithTopology(topology *clusterv1.Topology) *ClusterBuilder {
 	c.topology = topology
@@ -110,6 +117,9 @@ func (c *ClusterBuilder) Build() *clusterv1.Cluster {
 	if c.controlPlane != nil {
 		obj.Spec.ControlPlaneRef = objToRef(c.controlPlane)
 	}
+	if c.managedEtcd != nil {
+		obj.Spec.ManagedExternalEtcdRef = objToRef(c.managedEtcd)
+	}
 	return obj
 }
 
diff --git a/internal/test/builder/etcd.go b/internal/test/builder/etcd.go
new file mode 100644
index 000000000..9c187575f
--- /dev/null
+++ b/internal/test/builder/etcd.go
@@ -0,0 +1,80 @@
+/*
+Copyright 2021 The Kubernetes Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package builder
+
+import (
+	apiextensionsv1 "k8s.io/apiextensions-apiserver/pkg/apis/apiextensions/v1"
+	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
+	"k8s.io/apimachinery/pkg/runtime/schema"
+)
+
+var (
+	// EtcdGroupVersion is group version used for control plane objects.
+	EtcdGroupVersion = schema.GroupVersion{Group: "etcd.cluster.x-k8s.io", Version: "v1beta1"}
+
+	// GenericEtcdKind is the Kind for the GenericEtcd.
+	GenericEtcdKind = "GenericEtcd"
+	// GenericEtcdCRD is a generic control plane CRD.
+	GenericEtcdCRD = testEtcdCRD(EtcdGroupVersion.WithKind(GenericEtcdKind))
+)
+
+func testEtcdCRD(gvk schema.GroupVersionKind) *apiextensionsv1.CustomResourceDefinition {
+	return generateCRD(gvk, map[string]apiextensionsv1.JSONSchemaProps{
+		"metadata": {
+			// NOTE: in CRD there is only a partial definition of metadata schema.
+			// Ref https://github.com/kubernetes-sigs/controller-tools/blob/59485af1c1f6a664655dad49543c474bb4a0d2a2/pkg/crd/gen.go#L185
+			Type: "object",
+		},
+		"spec": etcdSpecSchema,
+		"status": {
+			Type: "object",
+			Properties: map[string]apiextensionsv1.JSONSchemaProps{
+				// mandatory fields from the Cluster API contract
+				"ready":       {Type: "boolean"},
+				"initialized": {Type: "boolean"},
+				"endpoints":   {Type: "string"},
+			},
+		},
+	})
+}
+
+var etcdSpecSchema = apiextensionsv1.JSONSchemaProps{
+	Type:       "object",
+	Properties: map[string]apiextensionsv1.JSONSchemaProps{},
+}
+
+// EtcdPlaneBuilder holds the variables and objects needed to build a generic object for cluster.spec.ManagedExternalEtcdRef.
+type EtcdPlaneBuilder struct {
+	obj *unstructured.Unstructured
+}
+
+// Etcd returns a EtcdBuilder with the given name and Namespace.
+func Etcd(namespace, name string) *EtcdPlaneBuilder {
+	obj := &unstructured.Unstructured{}
+	obj.SetAPIVersion(EtcdGroupVersion.String())
+	obj.SetKind(GenericEtcdKind)
+	obj.SetNamespace(namespace)
+	obj.SetName(name)
+	return &EtcdPlaneBuilder{
+		obj: obj,
+	}
+}
+
+// Build generates an Unstructured object from the information passed to the EtcdPlaneBuilder.
+func (c *EtcdPlaneBuilder) Build() *unstructured.Unstructured {
+	return c.obj
+}
diff --git a/util/annotations/helpers.go b/util/annotations/helpers.go
index 47dc7fc6b..e611502ae 100644
--- a/util/annotations/helpers.go
+++ b/util/annotations/helpers.go
@@ -83,6 +83,11 @@ func AddAnnotations(o metav1.Object, desired map[string]string) bool {
 	return hasChanged
 }
 
+// HasAnnotation returns true if the object has the specified annotation.
+func HasAnnotation(o metav1.Object, annotation string) bool {
+	return hasAnnotation(o, annotation)
+}
+
 // hasAnnotation returns true if the object has the specified annotation.
 func hasAnnotation(o metav1.Object, annotation string) bool {
 	annotations := o.GetAnnotations()
-- 
2.46.0

