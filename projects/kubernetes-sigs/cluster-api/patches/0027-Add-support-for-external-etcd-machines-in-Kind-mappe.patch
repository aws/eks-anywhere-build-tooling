From 23e5886de2cc8ee0bb0efc24cf7b2f321ba237e1 Mon Sep 17 00:00:00 2001
From: Prow Bot <prow@amazonaws.com>
Date: Wed, 16 Aug 2023 19:58:01 -0700
Subject: [PATCH 27/39] Add support for external etcd machines in Kind mapper

---
 .../client/cluster/objectgraph_test.go        |  2 +-
 .../dockermachinepool_controller_phases.go    |  2 +-
 .../backends/docker/dockermachine_backend.go  | 32 +++++----
 .../docker/internal/docker/machine.go         | 68 +++++++++++++------
 4 files changed, 67 insertions(+), 37 deletions(-)

diff --git a/cmd/clusterctl/client/cluster/objectgraph_test.go b/cmd/clusterctl/client/cluster/objectgraph_test.go
index 28e29eca0..808fe1199 100644
--- a/cmd/clusterctl/client/cluster/objectgraph_test.go
+++ b/cmd/clusterctl/client/cluster/objectgraph_test.go
@@ -2182,7 +2182,7 @@ func TestObjectGraph_DiscoveryByNamespace(t *testing.T) {
 		{
 			name: "two clusters with external force object, read only 1 cluster & both external objects",
 			args: args{
-				cluster: "cluster1", // read only from ns1
+				namespace: "ns1", // read only from ns1
 				objs: func() []client.Object {
 					objs := []client.Object{}
 					objs = append(objs, test.NewFakeCluster("ns1", "cluster1").Objs()...)
diff --git a/test/infrastructure/docker/exp/internal/controllers/dockermachinepool_controller_phases.go b/test/infrastructure/docker/exp/internal/controllers/dockermachinepool_controller_phases.go
index f01653849..ca3c2d1cc 100644
--- a/test/infrastructure/docker/exp/internal/controllers/dockermachinepool_controller_phases.go
+++ b/test/infrastructure/docker/exp/internal/controllers/dockermachinepool_controller_phases.go
@@ -98,7 +98,7 @@ func createDockerContainer(ctx context.Context, name string, cluster *clusterv1.
 	}
 
 	log.Info("Creating container for machinePool", "name", name, "MachinePool", klog.KObj(machinePool))
-	if err := externalMachine.Create(ctx, dockerMachinePool.Spec.Template.CustomImage, constants.WorkerNodeRoleValue, machinePool.Spec.Template.Spec.Version, labels, dockerMachinePool.Spec.Template.ExtraMounts); err != nil {
+	if err := externalMachine.Create(ctx, dockerMachinePool.Spec.Template.CustomImage, constants.WorkerNodeRoleValue, machinePool.Spec.Template.Spec.Version, labels, dockerMachinePool.Spec.Template.ExtraMounts, false); err != nil {
 		return errors.Wrapf(err, "failed to create docker machine with name %s", name)
 	}
 	return nil
diff --git a/test/infrastructure/docker/internal/controllers/backends/docker/dockermachine_backend.go b/test/infrastructure/docker/internal/controllers/backends/docker/dockermachine_backend.go
index df08570e0..ffd19a7f6 100644
--- a/test/infrastructure/docker/internal/controllers/backends/docker/dockermachine_backend.go
+++ b/test/infrastructure/docker/internal/controllers/backends/docker/dockermachine_backend.go
@@ -197,7 +197,7 @@ func (r *MachineBackendReconciler) ReconcileNormal(ctx context.Context, cluster
 	if !externalMachine.Exists() {
 		// NOTE: FailureDomains don't mean much in CAPD since it's all local, but we are setting a label on
 		// each container, so we can check placement.
-		if err := externalMachine.Create(ctx, dockerMachine.Spec.Backend.Docker.CustomImage, role, machine.Spec.Version, docker.FailureDomainLabel(machine.Spec.FailureDomain), dockerMachine.Spec.Backend.Docker.ExtraMounts); err != nil {
+		if err := externalMachine.Create(ctx, dockerMachine.Spec.Backend.Docker.CustomImage, role, machine.Spec.Version, docker.FailureDomainLabel(machine.Spec.FailureDomain), dockerMachine.Spec.Backend.Docker.ExtraMounts, util.IsEtcdMachine(machine)); err != nil {
 			return ctrl.Result{}, errors.Wrap(err, "failed to create worker DockerMachine")
 		}
 	}
@@ -294,7 +294,7 @@ func (r *MachineBackendReconciler) ReconcileNormal(ctx context.Context, cluster
 			}()
 
 			// Run the bootstrap script. Simulates cloud-init/Ignition.
-			if err := externalMachine.ExecBootstrap(timeoutCtx, bootstrapData, format, version, dockerMachine.Spec.Backend.Docker.CustomImage); err != nil {
+			if err := externalMachine.ExecBootstrap(timeoutCtx, bootstrapData, format, version, dockerMachine.Spec.Backend.Docker.CustomImage, util.IsEtcdMachine(machine)); err != nil {
 				v1beta1conditions.MarkFalse(dockerMachine, infrav1.BootstrapExecSucceededV1Beta1Condition, infrav1.BootstrapFailedV1Beta1Reason, clusterv1.ConditionSeverityWarning, "Repeating bootstrap")
 				conditions.Set(dockerMachine, metav1.Condition{
 					Type:    infrav1.DevMachineDockerContainerBootstrapExecSucceededCondition,
@@ -340,23 +340,27 @@ func (r *MachineBackendReconciler) ReconcileNormal(ctx context.Context, cluster
 	// set to true after a control plane machine has a node ref. If we would requeue here in this case, the
 	// Machine will never get a node ref as ProviderID is required to set the node ref, so we would get a deadlock.
 	if cluster.Spec.ControlPlaneRef.IsDefined() &&
+		!util.IsEtcdMachine(machine) &&
 		!conditions.IsTrue(cluster, clusterv1.ClusterControlPlaneInitializedCondition) {
 		return ctrl.Result{RequeueAfter: 5 * time.Second}, nil
 	}
 
-	// Usually a cloud provider will do this, but there is no docker-cloud provider.
-	// Requeue if there is an error, as this is likely momentary load balancer
-	// state changes during control plane provisioning.
-	remoteClient, err := r.ClusterCache.GetClient(ctx, client.ObjectKeyFromObject(cluster))
-	if err != nil {
-		return ctrl.Result{}, errors.Wrap(err, "failed to generate workload cluster client")
-	}
-	if err := externalMachine.CloudProviderNodePatch(ctx, remoteClient, dockerMachine.Status.Addresses); err != nil {
-		if errors.As(err, &docker.ContainerNotRunningError{}) {
-			return ctrl.Result{}, errors.Wrap(err, "failed to patch the Kubernetes node with the machine providerID")
+	// In case of an etcd cluster, there is no concept of kubernetes node. So we can generate the node Provider ID and set it on machine spec directly
+	if !util.IsEtcdMachine(machine) {
+		// Usually a cloud provider will do this, but there is no docker-cloud provider.
+		// Requeue if there is an error, as this is likely momentary load balancer
+		// state changes during control plane provisioning.
+		remoteClient, err := r.ClusterCache.GetClient(ctx, client.ObjectKeyFromObject(cluster))
+		if err != nil {
+			return ctrl.Result{}, errors.Wrap(err, "failed to generate workload cluster client")
+		}
+		if err := externalMachine.CloudProviderNodePatch(ctx, remoteClient, dockerMachine.Status.Addresses); err != nil {
+			if errors.As(err, &docker.ContainerNotRunningError{}) {
+				return ctrl.Result{}, errors.Wrap(err, "failed to patch the Kubernetes node with the machine providerID")
+			}
+			log.Error(err, "Failed to patch the Kubernetes node with the machine providerID")
+			return ctrl.Result{RequeueAfter: 5 * time.Second}, nil
 		}
-		log.Error(err, "Failed to patch the Kubernetes node with the machine providerID")
-		return ctrl.Result{RequeueAfter: 5 * time.Second}, nil
 	}
 	// Set ProviderID so the Cluster API Machine Controller can pull it
 	dockerMachine.Spec.ProviderID = externalMachine.ProviderID()
diff --git a/test/infrastructure/docker/internal/docker/machine.go b/test/infrastructure/docker/internal/docker/machine.go
index 4d261813c..14ef9a5a2 100644
--- a/test/infrastructure/docker/internal/docker/machine.go
+++ b/test/infrastructure/docker/internal/docker/machine.go
@@ -48,6 +48,7 @@ import (
 	"sigs.k8s.io/cluster-api/test/infrastructure/docker/internal/provisioning/ignition"
 	"sigs.k8s.io/cluster-api/test/infrastructure/kind"
 	"sigs.k8s.io/cluster-api/util/patch"
+	versionutil "sigs.k8s.io/cluster-api/util/version"
 )
 
 var (
@@ -201,23 +202,36 @@ func (m *Machine) ContainerImage() string {
 }
 
 // Create creates a docker container hosting a Kubernetes node.
-func (m *Machine) Create(ctx context.Context, image string, role string, version string, labels map[string]string, mounts []infrav1.Mount) error {
+func (m *Machine) Create(ctx context.Context, image string, role string, version string, labels map[string]string, mounts []infrav1.Mount, isEtcdMachine bool) error {
 	log := ctrl.LoggerFrom(ctx)
 
 	// Create if not exists.
 	if m.container == nil {
 		var err error
 
-		// Get the KindMapping for the target K8s version.
-		// NOTE: The KindMapping allows to select the most recent kindest/node image available, if any, as well as
-		// provide info about the mode to be used when starting the kindest/node image itself.
-		if version == "" {
-			return errors.New("cannot create a DockerMachine for a nil version")
-		}
+		var semVer semver.Version
 
-		semVer, err := semver.ParseTolerant(version)
-		if err != nil {
-			return errors.Wrap(err, "failed to parse DockerMachine version")
+		// External etcd machines do not set a version field in the machine.Spec.Version.
+		// So we are parsing the Kubernetes semantic version from the Kind node tag and
+		// using that to get the Kind Mapping.
+		if isEtcdMachine {
+			nodeImageTag := strings.Split(image, ":")[1]
+			semVer, err = versionutil.ParseMajorMinorPatch(nodeImageTag)
+			if err != nil {
+				return errors.Wrap(err, "failed to parse semantic version from image tag")
+			}
+		} else {
+			// Parse the semver from the Spec.Version if not empty and get the KindMapping using the semver.
+			// NOTE: The KindMapping allows to select the most recent kindest/node image available, if any, as well as
+			// provide info about the mode to be used when starting the kindest/node image itself.
+			if version == "" {
+				return errors.New("cannot create a DockerMachine for an empty version")
+			}
+
+			semVer, err = semver.Parse(strings.TrimPrefix(version, "v"))
+			if err != nil {
+				return errors.Wrap(err, "failed to parse DockerMachine version")
+			}
 		}
 
 		kindMapping := kind.GetMapping(semVer, image)
@@ -329,23 +343,35 @@ func (m *Machine) PreloadLoadImages(ctx context.Context, images []string) error
 }
 
 // ExecBootstrap runs bootstrap on a node, this is generally `kubeadm <init|join>`.
-func (m *Machine) ExecBootstrap(ctx context.Context, data string, format bootstrapv1.Format, version string, image string) error {
+func (m *Machine) ExecBootstrap(ctx context.Context, data string, format bootstrapv1.Format, version string, image string, isEtcdMachine bool) error {
 	log := ctrl.LoggerFrom(ctx)
 
 	if m.container == nil {
 		return errors.New("unable to set ExecBootstrap. the container hosting this machine does not exists")
 	}
 
-	// Get the kindMapping for the target K8s version.
-	// NOTE: The kindMapping allows to select the most recent kindest/node image available, if any, as well as
-	// provide info about the mode to be used when starting the kindest/node image itself.
-	if version == "" {
-		return errors.New("cannot create a DockerMachine for a nil version")
-	}
-
-	semVer, err := semver.ParseTolerant(version)
-	if err != nil {
-		return errors.Wrap(err, "failed to parse DockerMachine version")
+	var err error
+	var semVer semver.Version
+	// External etcd machines do not set a version field in the machine.Spec.Version.
+	// So we are parsing the Kubernetes semantic version from the Kind node tag and
+	// using that to get the Kind Mapping.
+	if isEtcdMachine {
+		nodeImageTag := strings.Split(image, ":")[1]
+		semVer, err = versionutil.ParseMajorMinorPatch(nodeImageTag)
+		if err != nil {
+			return errors.Wrap(err, "failed to parse semantic version from image tag")
+		}
+	} else {
+		// Parse the semver from the Spec.Version if not empty and get the KindMapping using the semver.
+		// NOTE: The KindMapping allows to select the most recent kindest/node image available, if any, as well as
+		// provide info about the mode to be used when starting the kindest/node image itself.
+		if version == "" {
+			return errors.New("cannot create a DockerMachine for an empty version")
+		}
+		semVer, err = semver.Parse(strings.TrimPrefix(version, "v"))
+		if err != nil {
+			return errors.Wrap(err, "failed to parse DockerMachine version")
+		}
 	}
 
 	kindMapping := kind.GetMapping(semVer, image)
-- 
2.48.1

