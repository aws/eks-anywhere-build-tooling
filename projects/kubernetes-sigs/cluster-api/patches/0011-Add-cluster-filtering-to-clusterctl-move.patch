From edf8f036763d87a3786dd7c67b350afe6e22a28a Mon Sep 17 00:00:00 2001
From: Vignesh Goutham Ganesh <vgg@amazon.com>
Date: Tue, 16 May 2023 11:03:09 -0500
Subject: [PATCH 11/18] Add cluster filtering to clusterctl move

This patch adds comprehensive cluster move filtering functionality:
- Core filtering implementation for cluster selection
- Support for force-move label on objects without cluster tenants

Objects with force-move label and no cluster tenants are moved
regardless of cluster filtering. This enables moving external objects
that don't have explicit cluster ownership.
 cmd/clusterctl/client/cluster/mover_test.go   |  20 +-
 cmd/clusterctl/client/cluster/objectgraph.go  |  56 +++++-
 .../client/cluster/objectgraph_test.go        | 174 +++++++++++++++++-
 cmd/clusterctl/client/move.go                 |  10 +-
 cmd/clusterctl/client/move_test.go            |   8 +-
 cmd/clusterctl/cmd/move.go                    |   4 +
 7 files changed, 269 insertions(+), 34 deletions(-)
---
 cmd/clusterctl/client/cluster/mover.go        |  31 +++-
 cmd/clusterctl/client/cluster/mover_test.go   |  20 +-
 cmd/clusterctl/client/cluster/objectgraph.go  |  56 +++++-
 .../client/cluster/objectgraph_test.go        | 174 +++++++++++++++++-
 cmd/clusterctl/client/move.go                 |  10 +-
 cmd/clusterctl/client/move_test.go            |   8 +-
 cmd/clusterctl/cmd/move.go                    |   4 +
 .../webhooks/kubeadm_control_plane.go         |   2 +
 .../webhooks/kubeadm_control_plane_test.go    |  45 +++++
 9 files changed, 316 insertions(+), 34 deletions(-)

diff --git a/cmd/clusterctl/client/cluster/mover.go b/cmd/clusterctl/client/cluster/mover.go
index 7a39750..c8f8a0a 100644
--- a/cmd/clusterctl/client/cluster/mover.go
+++ b/cmd/clusterctl/client/cluster/mover.go
@@ -52,13 +52,16 @@ type ResourceMutatorFunc func(u *unstructured.Unstructured) error
 // ObjectMover defines methods for moving Cluster API objects to another management cluster.
 type ObjectMover interface {
 	// Move moves all the Cluster API objects existing in a namespace (or from all the namespaces if empty) to a target management cluster.
-	Move(ctx context.Context, namespace string, toCluster Client, dryRun bool, mutators ...ResourceMutatorFunc) error
+	// If `clusterName` is specified (not empty string), only objects belonging to the cluster will be moved.
+	Move(ctx context.Context, namespace string, toCluster Client, clusterName string, dryRun bool, mutators ...ResourceMutatorFunc) error
 
 	// ToDirectory writes all the Cluster API objects existing in a namespace (or from all the namespaces if empty) to a target directory.
-	ToDirectory(ctx context.Context, namespace string, directory string) error
+	// If `clusterName` is specified (not empty string), only objects belonging to the cluster will be moved.
+	ToDirectory(ctx context.Context, namespace string, directory, clusterName string) error
 
 	// FromDirectory reads all the Cluster API objects existing in a configured directory to a target management cluster.
-	FromDirectory(ctx context.Context, toCluster Client, directory string) error
+	// If `clusterName` is specified (not empty string), only objects belonging to the cluster will be moved.
+	FromDirectory(ctx context.Context, toCluster Client, directory, clusterName string) error
 }
 
 // objectMover implements the ObjectMover interface.
@@ -71,7 +74,7 @@ type objectMover struct {
 // ensure objectMover implements the ObjectMover interface.
 var _ ObjectMover = &objectMover{}
 
-func (o *objectMover) Move(ctx context.Context, namespace string, toCluster Client, dryRun bool, mutators ...ResourceMutatorFunc) error {
+func (o *objectMover) Move(ctx context.Context, namespace string, toCluster Client, clusterName string, dryRun bool, mutators ...ResourceMutatorFunc) error {
 	log := logf.Log
 	log.Info("Performing move...")
 	o.dryRun = dryRun
@@ -88,7 +91,7 @@ func (o *objectMover) Move(ctx context.Context, namespace string, toCluster Clie
 		}
 	}
 
-	objectGraph, err := o.getObjectGraph(ctx, namespace)
+	objectGraph, err := o.getObjectGraph(ctx, namespace, clusterName)
 	if err != nil {
 		return errors.Wrap(err, "failed to get object graph")
 	}
@@ -102,11 +105,11 @@ func (o *objectMover) Move(ctx context.Context, namespace string, toCluster Clie
 	return o.move(ctx, objectGraph, proxy, mutators...)
 }
 
-func (o *objectMover) ToDirectory(ctx context.Context, namespace string, directory string) error {
+func (o *objectMover) ToDirectory(ctx context.Context, namespace string, directory, clusterName string) error {
 	log := logf.Log
 	log.Info("Moving to directory...")
 
-	objectGraph, err := o.getObjectGraph(ctx, namespace)
+	objectGraph, err := o.getObjectGraph(ctx, namespace, clusterName)
 	if err != nil {
 		return errors.Wrap(err, "failed to get object graph")
 	}
@@ -114,7 +117,7 @@ func (o *objectMover) ToDirectory(ctx context.Context, namespace string, directo
 	return o.toDirectory(ctx, objectGraph, directory)
 }
 
-func (o *objectMover) FromDirectory(ctx context.Context, toCluster Client, directory string) error {
+func (o *objectMover) FromDirectory(ctx context.Context, toCluster Client, directory, clusterName string) error {
 	log := logf.Log
 	log.Info("Moving from directory...")
 
@@ -148,6 +151,14 @@ func (o *objectMover) FromDirectory(ctx context.Context, toCluster Client, direc
 	// Check whether nodes are not included in GVK considered for fromDirectory.
 	objectGraph.checkVirtualNode()
 
+	// Filter and remove nodes in the graph that do not belong to cluster
+	if clusterName != "" {
+		err = objectGraph.filterCluster(clusterName)
+		if err != nil {
+			return errors.Wrap(err, "failed to filter for cluster")
+		}
+	}
+
 	// Restore the objects to the target cluster.
 	proxy := toCluster.Proxy()
 
@@ -185,7 +196,7 @@ func (o *objectMover) filesToObjs(dir string) ([]unstructured.Unstructured, erro
 	return objs, nil
 }
 
-func (o *objectMover) getObjectGraph(ctx context.Context, namespace string) (*objectGraph, error) {
+func (o *objectMover) getObjectGraph(ctx context.Context, namespace, clusterName string) (*objectGraph, error) {
 	objectGraph := newObjectGraph(o.fromProxy, o.fromProviderInventory)
 
 	// Gets all the types defined by the CRDs installed by clusterctl plus the ConfigMap/Secret core types.
@@ -197,7 +208,7 @@ func (o *objectMover) getObjectGraph(ctx context.Context, namespace string) (*ob
 	// Discovery the object graph for the selected types:
 	// - Nodes are defined the Kubernetes objects (Clusters, Machines etc.) identified during the discovery process.
 	// - Edges are derived by the OwnerReferences between nodes.
-	if err := objectGraph.Discovery(ctx, namespace); err != nil {
+	if err := objectGraph.Discovery(ctx, namespace, clusterName); err != nil {
 		return nil, errors.Wrap(err, "failed to discover the object graph")
 	}
 
diff --git a/cmd/clusterctl/client/cluster/mover_test.go b/cmd/clusterctl/client/cluster/mover_test.go
index e727930..9472d65 100644
--- a/cmd/clusterctl/client/cluster/mover_test.go
+++ b/cmd/clusterctl/client/cluster/mover_test.go
@@ -746,7 +746,7 @@ func Test_objectMover_backupTargetObject(t *testing.T) {
 			g.Expect(graph.getDiscoveryTypes(ctx)).To(Succeed())
 
 			// trigger discovery the content of the source cluster
-			g.Expect(graph.Discovery(ctx, "")).To(Succeed())
+			g.Expect(graph.Discovery(ctx, "", "")).To(Succeed())
 
 			// Run backupTargetObject on nodes in graph
 			mover := objectMover{
@@ -829,7 +829,7 @@ func Test_objectMover_restoreTargetObject(t *testing.T) {
 			g.Expect(graph.getDiscoveryTypes(ctx)).To(Succeed())
 
 			// trigger discovery the content of the source cluster
-			g.Expect(graph.Discovery(ctx, "")).To(Succeed())
+			g.Expect(graph.Discovery(ctx, "", "")).To(Succeed())
 
 			// gets a fakeProxy to an empty cluster with all the required CRDs
 			toProxy := getFakeProxyWithCRDs()
@@ -937,7 +937,7 @@ func Test_objectMover_toDirectory(t *testing.T) {
 			g.Expect(graph.getDiscoveryTypes(ctx)).To(Succeed())
 
 			// trigger discovery the content of the source cluster
-			g.Expect(graph.Discovery(ctx, "")).To(Succeed())
+			g.Expect(graph.Discovery(ctx, "", "")).To(Succeed())
 
 			// Run toDirectory
 			mover := objectMover{
@@ -1145,7 +1145,7 @@ func Test_getMoveSequence(t *testing.T) {
 			g.Expect(graph.getDiscoveryTypes(ctx)).To(Succeed())
 
 			// trigger discovery the content of the source cluster
-			g.Expect(graph.Discovery(ctx, "")).To(Succeed())
+			g.Expect(graph.Discovery(ctx, "", "")).To(Succeed())
 
 			moveSequence := getMoveSequence(graph)
 			g.Expect(moveSequence.groups).To(HaveLen(len(tt.wantMoveGroups)))
@@ -1178,7 +1178,7 @@ func Test_objectMover_move_dryRun(t *testing.T) {
 			g.Expect(graph.getDiscoveryTypes(ctx)).To(Succeed())
 
 			// trigger discovery the content of the source cluster
-			g.Expect(graph.Discovery(ctx, "")).To(Succeed())
+			g.Expect(graph.Discovery(ctx, "", "")).To(Succeed())
 
 			// gets a fakeProxy to an empty cluster with all the required CRDs
 			toProxy := getFakeProxyWithCRDs()
@@ -1253,7 +1253,7 @@ func Test_objectMover_move(t *testing.T) {
 			g.Expect(graph.getDiscoveryTypes(ctx)).To(Succeed())
 
 			// trigger discovery the content of the source cluster
-			g.Expect(graph.Discovery(ctx, "")).To(Succeed())
+			g.Expect(graph.Discovery(ctx, "", "")).To(Succeed())
 
 			// gets a fakeProxy to an empty cluster with all the required CRDs
 			toProxy := getFakeProxyWithCRDs()
@@ -1365,7 +1365,7 @@ func Test_objectMover_move_with_Mutator(t *testing.T) {
 			g.Expect(graph.getDiscoveryTypes(ctx)).To(Succeed())
 
 			// trigger discovery the content of the source cluster
-			g.Expect(graph.Discovery(ctx, "")).To(Succeed())
+			g.Expect(graph.Discovery(ctx, "", "")).To(Succeed())
 
 			// gets a fakeProxy to an empty cluster with all the required CRDs
 			toProxy := getFakeProxyWithCRDs()
@@ -1653,7 +1653,7 @@ func Test_objectMover_checkProvisioningCompleted(t *testing.T) {
 			g.Expect(graph.getDiscoveryTypes(ctx)).To(Succeed())
 
 			// trigger discovery the content of the source cluster
-			g.Expect(graph.Discovery(ctx, "")).To(Succeed())
+			g.Expect(graph.Discovery(ctx, "", "")).To(Succeed())
 
 			o := &objectMover{
 				fromProxy: graph.proxy,
@@ -1910,7 +1910,7 @@ func Test_objectMoverService_ensureNamespaces(t *testing.T) {
 			g.Expect(graph.getDiscoveryTypes(ctx)).To(Succeed())
 
 			// Trigger discovery the content of the source cluster
-			g.Expect(graph.Discovery(ctx, "")).To(Succeed())
+			g.Expect(graph.Discovery(ctx, "", "")).To(Succeed())
 
 			mover := objectMover{
 				fromProxy: graph.proxy,
@@ -2436,7 +2436,7 @@ func TestWaitReadyForMove(t *testing.T) {
 			g.Expect(graph.getDiscoveryTypes(ctx)).To(Succeed())
 
 			// trigger discovery the content of the source cluster
-			g.Expect(graph.Discovery(ctx, "")).To(Succeed())
+			g.Expect(graph.Discovery(ctx, "", "")).To(Succeed())
 
 			backoff := wait.Backoff{
 				Steps: 1,
diff --git a/cmd/clusterctl/client/cluster/objectgraph.go b/cmd/clusterctl/client/cluster/objectgraph.go
index 83bd433..5632a05 100644
--- a/cmd/clusterctl/client/cluster/objectgraph.go
+++ b/cmd/clusterctl/client/cluster/objectgraph.go
@@ -43,9 +43,11 @@ import (
 	secretutil "sigs.k8s.io/cluster-api/util/secret"
 )
 
-const clusterTopologyNameKey = "cluster.spec.topology.class"
-const clusterTopologyNamespaceKey = "cluster.spec.topology.classNamespace"
-const clusterResourceSetBindingClusterNameKey = "clusterresourcesetbinding.spec.clustername"
+const (
+	clusterTopologyNameKey                  = "cluster.spec.topology.class"
+	clusterTopologyNamespaceKey             = "cluster.spec.topology.classNamespace"
+	clusterResourceSetBindingClusterNameKey = "clusterresourcesetbinding.spec.clustername"
+)
 
 type empty struct{}
 
@@ -428,8 +430,8 @@ func getCRDList(ctx context.Context, proxy Proxy, crdList *apiextensionsv1.Custo
 }
 
 // Discovery reads all the Kubernetes objects existing in a namespace (or in all namespaces if empty) for the types received in input, and then adds
-// everything to the objects graph.
-func (o *objectGraph) Discovery(ctx context.Context, namespace string) error {
+// everything to the objects graph. Filters for objects only belonging to specific cluster if provided.
+func (o *objectGraph) Discovery(ctx context.Context, namespace, clusterName string) error {
 	log := logf.Log
 	log.Info("Discovering Cluster API objects")
 
@@ -559,10 +561,54 @@ func (o *objectGraph) Discovery(ctx context.Context, namespace string) error {
 	// Completes the graph by setting for each node the list of tenants the node belongs to.
 	o.setTenants()
 
+	// Filter and remove nodes in the graph that do not belong to cluster
+	if clusterName != "" {
+		return o.filterCluster(clusterName)
+	}
+
 	// Ensure objects which are referenced across namespaces are not deleted.
 	return o.setShouldNotDelete(ctx, namespace)
 }
 
+// filterCluster removes all objects but provided cluster and its dependents and soft-dependents
+func (o *objectGraph) filterCluster(clusterName string) error {
+	for _, object := range o.getNodes() {
+
+		hasFilterCluster := false
+		var clusterTenants []string
+		for tenant := range object.tenant {
+			if tenant.identity.GroupVersionKind().GroupKind() == clusterv1.GroupVersion.WithKind("Cluster").GroupKind() {
+				clusterTenants = append(clusterTenants, tenant.identity.Name)
+				if tenant.identity.Name == clusterName {
+					hasFilterCluster = true
+				}
+			}
+		}
+
+		// Return error only when node has more than 1 cluster tenant and one of those cluster tenant is the clusterName
+		// being filtered for. This is to prevent moving an object that more than one cluster is dependent on.
+		if hasFilterCluster && len(clusterTenants) > 1 {
+			return fmt.Errorf("resource %s is a dependent of clusters %s. Only one cluster dependent allowed",
+				object.identity.Name, strings.Join(clusterTenants, ","))
+		}
+
+		// CAPI has a force move label that can be used on CRD to identify objects to be moved CRDs with that label has
+		// `forceMove` set to true. Only move forceMove nodes if and only if they do not have a cluster Tenant.
+		// If an object has `forceMove` and also has a clusterTenant, the object has clear owner ref assigned and can be
+		// dropped if the cluster being filtered is not part of its tenants
+		if len(clusterTenants) == 0 && object.forceMove {
+			continue
+		}
+
+		if !hasFilterCluster {
+			if _, ok := o.uidToNode[object.identity.UID]; ok {
+				delete(o.uidToNode, object.identity.UID)
+			}
+		}
+	}
+	return nil
+}
+
 // fetchRef collects specified reference and adds to moved objects.
 func (o *objectGraph) fetchRef(ctx context.Context, opts wait.Backoff, ref *corev1.ObjectReference) (*unstructured.Unstructured, error) {
 	if ref == nil {
diff --git a/cmd/clusterctl/client/cluster/objectgraph_test.go b/cmd/clusterctl/client/cluster/objectgraph_test.go
index 11418ed..28e29ec 100644
--- a/cmd/clusterctl/client/cluster/objectgraph_test.go
+++ b/cmd/clusterctl/client/cluster/objectgraph_test.go
@@ -1799,7 +1799,7 @@ func TestObjectGraph_Discovery(t *testing.T) {
 			g.Expect(err).ToNot(HaveOccurred())
 
 			// finally test discovery
-			err = graph.Discovery(ctx, "")
+			err = graph.Discovery(ctx, "", "")
 			if tt.wantErr {
 				g.Expect(err).To(HaveOccurred())
 				return
@@ -2179,6 +2179,49 @@ func TestObjectGraph_DiscoveryByNamespace(t *testing.T) {
 				},
 			},
 		},
+		{
+			name: "two clusters with external force object, read only 1 cluster & both external objects",
+			args: args{
+				cluster: "cluster1", // read only from ns1
+				objs: func() []client.Object {
+					objs := []client.Object{}
+					objs = append(objs, test.NewFakeCluster("ns1", "cluster1").Objs()...)
+					objs = append(objs, test.NewFakeExternalObject("ns1", "externalObject1").Objs()...)
+					objs = append(objs, test.NewFakeCluster("ns1", "cluster2").Objs()...)
+					objs = append(objs, test.NewFakeExternalObject("ns2", "externalObject2").Objs()...)
+					return objs
+				}(),
+			},
+			want: wantGraph{
+				nodes: map[string]wantGraphItem{
+					"cluster.x-k8s.io/v1beta2, Kind=Cluster, ns1/cluster1": {
+						forceMove:          true,
+						forceMoveHierarchy: true,
+					},
+					"infrastructure.cluster.x-k8s.io/v1beta2, Kind=GenericInfrastructureCluster, ns1/cluster1": {
+						owners: []string{
+							"cluster.x-k8s.io/v1beta2, Kind=Cluster, ns1/cluster1",
+						},
+					},
+					"/v1, Kind=Secret, ns1/cluster1-ca": {
+						softOwners: []string{
+							"cluster.x-k8s.io/v1beta2, Kind=Cluster, ns1/cluster1", // NB. this secret is not linked to the cluster through owner ref
+						},
+					},
+					"/v1, Kind=Secret, ns1/cluster1-kubeconfig": {
+						owners: []string{
+							"cluster.x-k8s.io/v1beta2, Kind=Cluster, ns1/cluster1",
+						},
+					},
+					"external.cluster.x-k8s.io/v1beta2, Kind=GenericExternalObject, ns1/externalObject1": {
+						forceMove: true,
+					},
+					"external.cluster.x-k8s.io/v1beta2, Kind=GenericExternalObject, ns2/externalObject2": {
+						forceMove: true,
+					},
+				},
+			},
+		},
 	}
 
 	for _, tt := range tests {
@@ -2195,7 +2238,134 @@ func TestObjectGraph_DiscoveryByNamespace(t *testing.T) {
 			g.Expect(err).ToNot(HaveOccurred())
 
 			// finally test discovery
-			err = graph.Discovery(ctx, tt.args.namespace)
+			err = graph.Discovery(ctx, tt.args.namespace, "")
+			if tt.wantErr {
+				g.Expect(err).To(HaveOccurred())
+				return
+			}
+
+			g.Expect(err).ToNot(HaveOccurred())
+			assertGraph(t, graph, tt.want)
+		})
+	}
+}
+
+func TestObjectGraph_DiscoveryByCluster(t *testing.T) {
+	type args struct {
+		cluster string
+		objs    []client.Object
+	}
+	tests := []struct {
+		name    string
+		args    args
+		want    wantGraph
+		wantErr bool
+	}{
+		{
+			name: "two clusters, read both",
+			args: args{
+				cluster: "", // read all the namespaces
+				objs: func() []client.Object {
+					objs := []client.Object{}
+					objs = append(objs, test.NewFakeCluster("ns1", "cluster1").Objs()...)
+					objs = append(objs, test.NewFakeCluster("ns2", "cluster2").Objs()...)
+					return objs
+				}(),
+			},
+			want: wantGraph{
+				nodes: map[string]wantGraphItem{
+					"cluster.x-k8s.io/v1beta2, Kind=Cluster, ns1/cluster1": {
+						forceMove:          true,
+						forceMoveHierarchy: true,
+					},
+					"infrastructure.cluster.x-k8s.io/v1beta2, Kind=GenericInfrastructureCluster, ns1/cluster1": {
+						owners: []string{
+							"cluster.x-k8s.io/v1beta2, Kind=Cluster, ns1/cluster1",
+						},
+					},
+					"/v1, Kind=Secret, ns1/cluster1-ca": {
+						softOwners: []string{
+							"cluster.x-k8s.io/v1beta2, Kind=Cluster, ns1/cluster1", // NB. this secret is not linked to the cluster through owner ref
+						},
+					},
+					"/v1, Kind=Secret, ns1/cluster1-kubeconfig": {
+						owners: []string{
+							"cluster.x-k8s.io/v1beta2, Kind=Cluster, ns1/cluster1",
+						},
+					},
+					"cluster.x-k8s.io/v1beta2, Kind=Cluster, ns2/cluster2": {
+						forceMove:          true,
+						forceMoveHierarchy: true,
+					},
+					"infrastructure.cluster.x-k8s.io/v1beta2, Kind=GenericInfrastructureCluster, ns2/cluster2": {
+						owners: []string{
+							"cluster.x-k8s.io/v1beta2, Kind=Cluster, ns2/cluster2",
+						},
+					},
+					"/v1, Kind=Secret, ns2/cluster2-ca": {
+						softOwners: []string{
+							"cluster.x-k8s.io/v1beta2, Kind=Cluster, ns2/cluster2", // NB. this secret is not linked to the cluster through owner ref
+						},
+					},
+					"/v1, Kind=Secret, ns2/cluster2-kubeconfig": {
+						owners: []string{
+							"cluster.x-k8s.io/v1beta2, Kind=Cluster, ns2/cluster2",
+						},
+					},
+				},
+			},
+		},
+		{
+			name: "two clusters, read only 1",
+			args: args{
+				cluster: "cluster1", // read only from ns1
+				objs: func() []client.Object {
+					objs := []client.Object{}
+					objs = append(objs, test.NewFakeCluster("ns1", "cluster1").Objs()...)
+					objs = append(objs, test.NewFakeCluster("ns1", "cluster2").Objs()...)
+					return objs
+				}(),
+			},
+			want: wantGraph{
+				nodes: map[string]wantGraphItem{
+					"cluster.x-k8s.io/v1beta2, Kind=Cluster, ns1/cluster1": {
+						forceMove:          true,
+						forceMoveHierarchy: true,
+					},
+					"infrastructure.cluster.x-k8s.io/v1beta2, Kind=GenericInfrastructureCluster, ns1/cluster1": {
+						owners: []string{
+							"cluster.x-k8s.io/v1beta2, Kind=Cluster, ns1/cluster1",
+						},
+					},
+					"/v1, Kind=Secret, ns1/cluster1-ca": {
+						softOwners: []string{
+							"cluster.x-k8s.io/v1beta2, Kind=Cluster, ns1/cluster1", // NB. this secret is not linked to the cluster through owner ref
+						},
+					},
+					"/v1, Kind=Secret, ns1/cluster1-kubeconfig": {
+						owners: []string{
+							"cluster.x-k8s.io/v1beta2, Kind=Cluster, ns1/cluster1",
+						},
+					},
+				},
+			},
+		},
+	}
+
+	for _, tt := range tests {
+		t.Run(tt.name, func(t *testing.T) {
+			g := NewWithT(t)
+			ctx := context.Background()
+
+			// Create an objectGraph bound to a source cluster with all the CRDs for the types involved in the test.
+			graph := getObjectGraphWithObjs(tt.args.objs)
+
+			// Get all the types to be considered for discovery
+			err := graph.getDiscoveryTypes(ctx)
+			g.Expect(err).ToNot(HaveOccurred())
+
+			// finally test discovery
+			err = graph.Discovery(ctx, "", tt.args.cluster)
 			if tt.wantErr {
 				g.Expect(err).To(HaveOccurred())
 				return
diff --git a/cmd/clusterctl/client/move.go b/cmd/clusterctl/client/move.go
index 6d5299c..977f8b8 100644
--- a/cmd/clusterctl/client/move.go
+++ b/cmd/clusterctl/client/move.go
@@ -39,6 +39,10 @@ type MoveOptions struct {
 	// namespace will be used.
 	Namespace string
 
+	// ClusterName defines the name of the workload cluster and its dependent objects to be moved. If unspecified,
+	// all the clusters will be moved.
+	ClusterName string
+
 	// ExperimentalResourceMutatorFn accepts any number of resource mutator functions that are applied on all resources being moved.
 	// This is an experimental feature and is exposed only from the library and not (yet) through the CLI.
 	ExperimentalResourceMutators []cluster.ResourceMutatorFunc
@@ -99,7 +103,7 @@ func (c *clusterctlClient) move(ctx context.Context, options MoveOptions) error
 		}
 	}
 
-	return fromCluster.ObjectMover().Move(ctx, options.Namespace, toCluster, options.DryRun, options.ExperimentalResourceMutators...)
+	return fromCluster.ObjectMover().Move(ctx, options.Namespace, toCluster, options.ClusterName, options.DryRun, options.ExperimentalResourceMutators...)
 }
 
 func (c *clusterctlClient) fromDirectory(ctx context.Context, options MoveOptions) error {
@@ -112,7 +116,7 @@ func (c *clusterctlClient) fromDirectory(ctx context.Context, options MoveOption
 		return err
 	}
 
-	return toCluster.ObjectMover().FromDirectory(ctx, toCluster, options.FromDirectory)
+	return toCluster.ObjectMover().FromDirectory(ctx, toCluster, options.FromDirectory, options.ClusterName)
 }
 
 func (c *clusterctlClient) toDirectory(ctx context.Context, options MoveOptions) error {
@@ -134,7 +138,7 @@ func (c *clusterctlClient) toDirectory(ctx context.Context, options MoveOptions)
 		return err
 	}
 
-	return fromCluster.ObjectMover().ToDirectory(ctx, options.Namespace, options.ToDirectory)
+	return fromCluster.ObjectMover().ToDirectory(ctx, options.Namespace, options.ToDirectory, options.ClusterName)
 }
 
 func (c *clusterctlClient) getClusterClient(ctx context.Context, kubeconfig Kubeconfig) (cluster.Client, error) {
diff --git a/cmd/clusterctl/client/move_test.go b/cmd/clusterctl/client/move_test.go
index 4299832..784c93d 100644
--- a/cmd/clusterctl/client/move_test.go
+++ b/cmd/clusterctl/client/move_test.go
@@ -297,11 +297,11 @@ type fakeObjectMover struct {
 	fromDirectoryErr error
 }
 
-func (f *fakeObjectMover) Move(_ context.Context, _ string, _ cluster.Client, _ bool, _ ...cluster.ResourceMutatorFunc) error {
+func (f *fakeObjectMover) Move(_ context.Context, _ string, _ cluster.Client, _ string, _ bool, _ ...cluster.ResourceMutatorFunc) error {
 	return f.moveErr
 }
 
-func (f *fakeObjectMover) ToDirectory(_ context.Context, _ string, _ string) error {
+func (f *fakeObjectMover) ToDirectory(_ context.Context, _ string, _, _ string) error {
 	return f.toDirectoryErr
 }
 
@@ -309,10 +309,10 @@ func (f *fakeObjectMover) Backup(_ context.Context, _ string, _ string) error {
 	return f.toDirectoryErr
 }
 
-func (f *fakeObjectMover) FromDirectory(_ context.Context, _ cluster.Client, _ string) error {
+func (f *fakeObjectMover) FromDirectory(_ context.Context, _ cluster.Client, _, _ string) error {
 	return f.fromDirectoryErr
 }
 
-func (f *fakeObjectMover) Restore(_ context.Context, _ cluster.Client, _ string) error {
+func (f *fakeObjectMover) Restore(_ context.Context, _ cluster.Client, _, _ string) error {
 	return f.fromDirectoryErr
 }
diff --git a/cmd/clusterctl/cmd/move.go b/cmd/clusterctl/cmd/move.go
index 9f0d166..3e82551 100644
--- a/cmd/clusterctl/cmd/move.go
+++ b/cmd/clusterctl/cmd/move.go
@@ -38,6 +38,7 @@ type moveOptions struct {
 	toKubeconfig          string
 	toKubeconfigContext   string
 	namespace             string
+	filterCluster         string
 	fromDirectory         string
 	toDirectory           string
 	dryRun                bool
@@ -90,6 +91,8 @@ func init() {
 		"Read Cluster API objects and all dependencies from a directory into a management cluster.")
 	moveCmd.Flags().StringVar(&mo.hideAPIWarnings, "hide-api-warnings", "default",
 		"Set of API server warnings to hide. Valid sets are \"default\" (includes metadata.finalizer warnings), \"all\" , and \"none\".")
+	moveCmd.Flags().StringVar(&mo.filterCluster, "filter-cluster", "",
+		"Name of the cluster to be moved. All the dependent objects will also be moved. If empty, all clusters will be moved")
 
 	moveCmd.MarkFlagsMutuallyExclusive("to-directory", "to-kubeconfig")
 	moveCmd.MarkFlagsMutuallyExclusive("from-directory", "to-directory")
@@ -167,6 +170,7 @@ func runMove() error {
 		FromDirectory:  mo.fromDirectory,
 		ToDirectory:    mo.toDirectory,
 		Namespace:      mo.namespace,
+		ClusterName:    mo.filterCluster,
 		DryRun:         mo.dryRun,
 	})
 }
diff --git a/controlplane/kubeadm/internal/webhooks/kubeadm_control_plane.go b/controlplane/kubeadm/internal/webhooks/kubeadm_control_plane.go
index e6cb129..b3f5916 100644
--- a/controlplane/kubeadm/internal/webhooks/kubeadm_control_plane.go
+++ b/controlplane/kubeadm/internal/webhooks/kubeadm_control_plane.go
@@ -163,6 +163,7 @@ func (webhook *KubeadmControlPlane) ValidateUpdate(_ context.Context, oldObj, ne
 		{spec, kubeadmConfigSpec, clusterConfiguration, controllerManager, "*"},
 		{spec, kubeadmConfigSpec, clusterConfiguration, scheduler},
 		{spec, kubeadmConfigSpec, clusterConfiguration, scheduler, "*"},
+		{spec, kubeadmConfigSpec, clusterConfiguration, "registryMirror", "*"},
 		{spec, kubeadmConfigSpec, clusterConfiguration, "certificateValidityPeriodDays"},
 		// spec.kubeadmConfigSpec.initConfiguration
 		{spec, kubeadmConfigSpec, initConfiguration, nodeRegistration},
@@ -181,6 +182,7 @@ func (webhook *KubeadmControlPlane) ValidateUpdate(_ context.Context, oldObj, ne
 		{spec, kubeadmConfigSpec, joinConfiguration, "bottlerocketControl", "*"},
 		{spec, kubeadmConfigSpec, joinConfiguration, "bottlerocketCustomBootstrapContainers"},
 		{spec, kubeadmConfigSpec, joinConfiguration, "bottlerocketSettings", "*"},
+		{spec, kubeadmConfigSpec, joinConfiguration, "registryMirror", "*"},
 		{spec, kubeadmConfigSpec, joinConfiguration, "pause", "*"},
 		{spec, kubeadmConfigSpec, joinConfiguration, nodeRegistration},
 		{spec, kubeadmConfigSpec, joinConfiguration, nodeRegistration, "*"},
diff --git a/controlplane/kubeadm/internal/webhooks/kubeadm_control_plane_test.go b/controlplane/kubeadm/internal/webhooks/kubeadm_control_plane_test.go
index 67312c3..7ce0d42 100644
--- a/controlplane/kubeadm/internal/webhooks/kubeadm_control_plane_test.go
+++ b/controlplane/kubeadm/internal/webhooks/kubeadm_control_plane_test.go
@@ -359,6 +359,10 @@ func TestKubeadmControlPlaneValidateUpdate(t *testing.T) {
 					},
 					CertificateValidityPeriodDays:   100,
 					CACertificateValidityPeriodDays: 365,
+					RegistryMirror: bootstrapv1.RegistryMirrorConfiguration{
+						Endpoint: "https://1.1.1.1:1111",
+						CACert:   "test-cert",
+					},
 				},
 				JoinConfiguration: bootstrapv1.JoinConfiguration{
 					NodeRegistration: bootstrapv1.NodeRegistrationOptions{
@@ -368,6 +372,10 @@ func TestKubeadmControlPlaneValidateUpdate(t *testing.T) {
 						ControlPlaneComponentHealthCheckSeconds: ptr.To[int32](10),
 						KubeletHealthCheckSeconds:               ptr.To[int32](40),
 					},
+					RegistryMirror: bootstrapv1.RegistryMirrorConfiguration{
+						Endpoint: "https://1.1.1.1:1111",
+						CACert:   "test-cert",
+					},
 				},
 				PreKubeadmCommands: []string{
 					"test", "foo",
@@ -785,6 +793,18 @@ func TestKubeadmControlPlaneValidateUpdate(t *testing.T) {
 	validUpdateJoinConfBRCustomBootstrapContainers := before.DeepCopy()
 	validUpdateJoinConfBRCustomBootstrapContainers.Spec.KubeadmConfigSpec.JoinConfiguration.BottlerocketCustomBootstrapContainers = []bootstrapv1.BottlerocketBootstrapContainer{{ImageRepository: "registry.k8s.io/bottlerocketbootstrap", ImageTag: "v1.1.0+new"}}
 
+	validUpdateClusterConfigRegistryMirrorCACert := before.DeepCopy()
+	validUpdateClusterConfigRegistryMirrorCACert.Spec.KubeadmConfigSpec.ClusterConfiguration.RegistryMirror.CACert = "foo:bar"
+
+	validUpdateJoinConfigRegistryMirrorCACert := before.DeepCopy()
+	validUpdateJoinConfigRegistryMirrorCACert.Spec.KubeadmConfigSpec.JoinConfiguration.RegistryMirror.CACert = "foo:bar"
+
+	validUpdateClusterConfigRegistryMirrorEndpoint := before.DeepCopy()
+	validUpdateClusterConfigRegistryMirrorEndpoint.Spec.KubeadmConfigSpec.ClusterConfiguration.RegistryMirror.Endpoint = "https://0.0.0.0:6443"
+
+	validUpdateJoinConfigRegistryMirrorEndpoint := before.DeepCopy()
+	validUpdateJoinConfigRegistryMirrorEndpoint.Spec.KubeadmConfigSpec.JoinConfiguration.RegistryMirror.Endpoint = "https://0.0.0.0:6443"
+
 	tests := []struct {
 		name                  string
 		enableIgnitionFeature bool
@@ -1208,6 +1228,31 @@ func TestKubeadmControlPlaneValidateUpdate(t *testing.T) {
 			before:    before,
 			kcp:       validUpdateJoinConfBRCustomBootstrapContainers,
 		},
+		{
+			name:      "should allow changes to join configuration registry mirror caCert",
+			expectErr: false,
+			before:    before,
+			kcp:       validUpdateJoinConfigRegistryMirrorCACert,
+		},
+		{
+			name:      "should allow changes to join configuration registry mirror endpoint",
+			expectErr: false,
+			before:    before,
+			kcp:       validUpdateJoinConfigRegistryMirrorEndpoint,
+		},
+		{
+			name:      "should allow changes to cluster configuration registry mirror caCert",
+			expectErr: false,
+			before:    before,
+			kcp:       validUpdateClusterConfigRegistryMirrorCACert,
+		},
+
+		{
+			name:      "should allow changes to cluster configuration registry mirror endpoint",
+			expectErr: false,
+			before:    before,
+			kcp:       validUpdateClusterConfigRegistryMirrorEndpoint,
+		},
 	}
 
 	for _, tt := range tests {
-- 
2.50.1 (Apple Git-155)

