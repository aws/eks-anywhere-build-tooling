#!/bin/bash
set -eux -o pipefail

# kubeadm + kubectl are in /opt/bin
export PATH="${PATH}:/opt/bin"

############################
# Modify files created by cloud-init script until we change capi to generate them the way we need for BR

# kube-vip needs the admin.conf to use the api server for leader election. we already symlink /etc/kubernetes to /var/lib/kubeadm so that it is persisted
yq -i e '.spec.volumes |= [{"hostPath":{"path":"/var/lib/kubeadm/admin.conf", "type": "File"}, "name":"kubeconfig"}]' /etc/kubernetes/manifests/kube-vip.yaml 

yq -i e 'select(.kind == "ClusterConfiguration").certificatesDir |= "/var/lib/kubeadm/pki"' /tmp/kubeadm.yaml

yq -i e 'select(.kind == "ClusterConfiguration").controllerManager.extraVolumes |= [{"name": "kubeconfig", "hostPath": "/var/lib/kubeadm/controller-manager.conf", "mountPath": "/etc/kubernetes/controller-manager.conf", "readOnly": true, "pathType": "File"}]' /tmp/kubeadm.yaml
yq -i e 'select(.kind == "ClusterConfiguration").scheduler.extraVolumes |= [{"name": "kubeconfig", "hostPath": "/var/lib/kubeadm/scheduler.conf", "mountPath": "/etc/kubernetes/scheduler.conf", "readOnly": true, "pathType": "File"}]' /tmp/kubeadm.yaml

###########################

# Generate keys and write the manifests we need.
kubeadm -v9 init phase certs all --config /tmp/kubeadm.yaml
kubeadm -v9 init phase kubeconfig all --config /tmp/kubeadm.yaml
kubeadm -v9 init phase control-plane all --config /tmp/kubeadm.yaml
kubeadm -v9 init phase etcd local --config /tmp/kubeadm.yaml

# Migrate all static pods from this host-container to the bottleroot host using the apiclient
for file in /etc/kubernetes/manifests/*.yaml; do
  filename=$(basename $file)
  pod="${filename%.*}" 
  cat $file
  apiclient set \
    "kubernetes.static-pods.${pod}.manifest"="$(base64 -w 0 $file)" \
    "kubernetes.static-pods.${pod}.enabled"=true
done


# At this point the admin.conf and cert files exists
API="$(awk '/server: /{ print $2 }' /etc/kubernetes/admin.conf)"
CA="$(base64 -w 0 /etc/kubernetes/pki/ca.crt)"

# Wait for Kubernetes API server to come up.
#https://github.com/kubernetes/kubeadm/blob/05f9b96cf83d73ed2177815487eba90517b66708/kinder/pkg/cluster/manager/actions/kubeadm-init.go#L167
while [ "$(curl -k https://localhost:6443/healthz -s -o /dev/null -w '%{http_code}')" != "200" ] ; do
  sleep 1
done

# If the api advertise url is different than localhost, like when using kube-vip, make sure it is accessible
while [ "$(curl -k $API/healthz -s -o /dev/null -w '%{http_code}')" != "200" ] ; do
  sleep 1
done

# Give static pods time to come up
# TODO: is there a way we could check if they are running?  We cant use kubectl because at this point in the process
# the kubelet hasnt joined so the pods arent "running" accoriding to the api server
sleep 30

# Set up the roles so our kubelet can bootstrap.
kubeadm -v9 init phase bootstrap-token --config /tmp/kubeadm.yaml

TOKEN="$(kubeadm token list -o jsonpath="{.token}")"
apiclient set \
  kubernetes.api-server="${API}" \
  kubernetes.cluster-certificate="${CA}" \
  kubernetes.bootstrap-token="${TOKEN}" \
  kubernetes.authentication-mode="tls" \
  kubernetes.standalone-mode=false


# Give the kubelet time to join the cluster.
sleep 5
# TODO: this gets ignored `Running in chroot, ignoring request.` maybe there is another way to validate this?
while ! systemctl is-active --quiet kubelet; do
  sleep 1
done

# Let kubeadm finish.
kubeadm -v9 init \
  --skip-phases preflight,kubelet-start,certs,kubeconfig,bootstrap-token,control-plane,etcd \
  --config /tmp/kubeadm.yaml

# Now that Core DNS is installed, find the cluster DNS IP.
DNS="$(kubectl --kubeconfig /etc/kubernetes/admin.conf get svc kube-dns -n kube-system -o jsonpath='{..clusterIP}')"
apiclient set kubernetes.cluster-dns-ip="${DNS}"
